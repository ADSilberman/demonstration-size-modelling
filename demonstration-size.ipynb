{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a282cba2",
   "metadata": {},
   "source": [
    "# Predicting Demonstration Size on Demographic Characteristics and Salience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca48a0",
   "metadata": {},
   "source": [
    "## GW DATS 6103: Introduction to Data Mining Final Project\n",
    "### Name: Alexander D. Silberman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2470d01",
   "metadata": {},
   "source": [
    "Note: Data is limited to 2017–2019, as COVID may substantially throw off the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524dd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from census import Census\n",
    "from us import states\n",
    "\n",
    "import censusdis.data as ced\n",
    "from censusdis.datasets import ACS1_PROFILE\n",
    "from censusdis import states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec124d",
   "metadata": {},
   "source": [
    "## Data Acquisition and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf888fcb",
   "metadata": {},
   "source": [
    "### Results Dataset: Crowd Counting Consortium Phase 1 (2017-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a925cb9",
   "metadata": {},
   "source": [
    "Our results dataset: Crowd Counting Consortium Phase 1 (2017-2020): https://ash.harvard.edu/programs/crowd-counting-consortium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8723fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_46648\\1640917472.py:1: DtypeWarning: Columns (6,22,31,32,33,34,35,36,37,38,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  crowd_data_orig = pd.read_csv(\"C:/Users/alexa/Code/GW DATS/6103 12 Intro to Data Mining/Final Project/data/crowd_counting_consortium_2017-2020/ccc_compiled_20172020.csv\",encoding_errors=\"replace\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>locality</th>\n",
       "      <th>state</th>\n",
       "      <th>location_detail</th>\n",
       "      <th>online</th>\n",
       "      <th>type</th>\n",
       "      <th>macroevent</th>\n",
       "      <th>actors</th>\n",
       "      <th>claims</th>\n",
       "      <th>valence</th>\n",
       "      <th>...</th>\n",
       "      <th>source_28</th>\n",
       "      <th>source_29</th>\n",
       "      <th>source_30</th>\n",
       "      <th>notes</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>resolved_locality</th>\n",
       "      <th>resolved_county</th>\n",
       "      <th>resolved_state</th>\n",
       "      <th>fips_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>Lafayette Square Park</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vigil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>for banning nuclear weapons, for peace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White House Peace Vigil continuous since June ...</td>\n",
       "      <td>38.907192</td>\n",
       "      <td>-77.036871</td>\n",
       "      <td>Washington</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>DC</td>\n",
       "      <td>11001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Mankato</td>\n",
       "      <td>MN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vigil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peace Vigil Mankato</td>\n",
       "      <td>for peace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>every Sunday since 2001</td>\n",
       "      <td>44.163578</td>\n",
       "      <td>-93.999400</td>\n",
       "      <td>Mankato</td>\n",
       "      <td>Blue Earth County</td>\n",
       "      <td>MN</td>\n",
       "      <td>27013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>MN</td>\n",
       "      <td>U.S. Bank Stadium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>general protestors</td>\n",
       "      <td>against the Dakota Access Pipeline, for indige...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hung banner from stadium roof during NFL game</td>\n",
       "      <td>44.977753</td>\n",
       "      <td>-93.265011</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>Hennepin County</td>\n",
       "      <td>MN</td>\n",
       "      <td>27053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Little Compton</td>\n",
       "      <td>RI</td>\n",
       "      <td>Town Green</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vigil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sakonnet Peace Alliance</td>\n",
       "      <td>for peace, for gun control, for climate action</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>every Sunday since 2003</td>\n",
       "      <td>41.510103</td>\n",
       "      <td>-71.171156</td>\n",
       "      <td>Little Compton</td>\n",
       "      <td>Newport County</td>\n",
       "      <td>RI</td>\n",
       "      <td>44005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Oak Ridge</td>\n",
       "      <td>TN</td>\n",
       "      <td>Y-12 National Security Complex</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vigil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oak Ridge Environmental Peace Alliance</td>\n",
       "      <td>for abolishing nuclear weapons</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>every Sunday since the late 1990s</td>\n",
       "      <td>36.010356</td>\n",
       "      <td>-84.269645</td>\n",
       "      <td>Oak Ridge</td>\n",
       "      <td>Anderson County</td>\n",
       "      <td>TN</td>\n",
       "      <td>47001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        locality state                 location_detail  online  \\\n",
       "0  2017-01-01      Washington    DC           Lafayette Square Park     0.0   \n",
       "1  2017-01-01         Mankato    MN                             NaN     0.0   \n",
       "2  2017-01-01     Minneapolis    MN               U.S. Bank Stadium     0.0   \n",
       "3  2017-01-01  Little Compton    RI                      Town Green     0.0   \n",
       "4  2017-01-01       Oak Ridge    TN  Y-12 National Security Complex     0.0   \n",
       "\n",
       "                   type macroevent                                  actors  \\\n",
       "0                 vigil        NaN                                     NaN   \n",
       "1                 vigil        NaN                     Peace Vigil Mankato   \n",
       "2  protest; banner drop        NaN                      general protestors   \n",
       "3                 vigil        NaN                 Sakonnet Peace Alliance   \n",
       "4                 vigil        NaN  Oak Ridge Environmental Peace Alliance   \n",
       "\n",
       "                                              claims  valence  ... source_28  \\\n",
       "0             for banning nuclear weapons, for peace      0.0  ...       NaN   \n",
       "1                                          for peace      0.0  ...       NaN   \n",
       "2  against the Dakota Access Pipeline, for indige...      1.0  ...       NaN   \n",
       "3     for peace, for gun control, for climate action      1.0  ...       NaN   \n",
       "4                     for abolishing nuclear weapons      0.0  ...       NaN   \n",
       "\n",
       "  source_29  source_30                                              notes  \\\n",
       "0       NaN        NaN  White House Peace Vigil continuous since June ...   \n",
       "1       NaN        NaN                            every Sunday since 2001   \n",
       "2       NaN        NaN      hung banner from stadium roof during NFL game   \n",
       "3       NaN        NaN                            every Sunday since 2003   \n",
       "4       NaN        NaN                  every Sunday since the late 1990s   \n",
       "\n",
       "         lat        lon resolved_locality       resolved_county  \\\n",
       "0  38.907192 -77.036871        Washington  District of Columbia   \n",
       "1  44.163578 -93.999400           Mankato     Blue Earth County   \n",
       "2  44.977753 -93.265011       Minneapolis       Hennepin County   \n",
       "3  41.510103 -71.171156    Little Compton        Newport County   \n",
       "4  36.010356 -84.269645         Oak Ridge       Anderson County   \n",
       "\n",
       "  resolved_state  fips_code  \n",
       "0             DC    11001.0  \n",
       "1             MN    27013.0  \n",
       "2             MN    27053.0  \n",
       "3             RI    44005.0  \n",
       "4             TN    47001.0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_data_orig = pd.read_csv(\"C:/Users/alexa/Code/GW DATS/6103 12 Intro to Data Mining/Final Project/data/crowd_counting_consortium_2017-2020/ccc_compiled_20172020.csv\",encoding_errors=\"replace\")\n",
    "crowd_data_orig.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcd9fc",
   "metadata": {},
   "source": [
    "Keep only data with a crowd size, information about what the crowd gathered for, and a FIPS code, and which aren't online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d2f2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                    0\n",
       "locality                0\n",
       "state                   0\n",
       "location_detail      2774\n",
       "online                  0\n",
       "                     ... \n",
       "lon                     2\n",
       "resolved_locality     345\n",
       "resolved_county      1929\n",
       "resolved_state         10\n",
       "fips_code               0\n",
       "Length: 62, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_data = crowd_data_orig[(crowd_data_orig[\"issues\"].notnull()) & \n",
    "                             (crowd_data_orig[\"size_mean\"].notnull()) & \n",
    "                             (crowd_data_orig[\"fips_code\"].notnull()) &\n",
    "                             (crowd_data_orig[\"online\"] == 0)]\n",
    "crowd_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4b058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_46648\\3801195337.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  crowd_data[\"year\"] = crowd_data[\"date\"].apply(lambda x:x[:4]).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2        2017\n",
       "8        2017\n",
       "13       2017\n",
       "14       2017\n",
       "16       2017\n",
       "         ... \n",
       "72155    2020\n",
       "72156    2020\n",
       "72167    2020\n",
       "72168    2020\n",
       "72172    2020\n",
       "Name: year, Length: 33811, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source_cols = np.array([\"source_\"]*30) + np.arange(1,31).astype(str)\n",
    "# crowd_data.drop(columns=source_cols, inplace=True)\n",
    "# crowd_data.head(5)\n",
    "# crowd_data[\"fips_code\"] = crowd_data[\"fips_code\"].astype(np.int16)\n",
    "# crowd_data[\"fips_code\"]\n",
    "\n",
    "crowd_data[\"year\"] = crowd_data[\"date\"].apply(lambda x:x[:4]).astype(int)\n",
    "crowd_data[\"year\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083677a",
   "metadata": {},
   "source": [
    "Limit to the years 2017 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2ab138",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_data = crowd_data[(crowd_data[\"year\"] == 2017) | (crowd_data[\"year\"] == 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6700d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 16764\n"
     ]
    }
   ],
   "source": [
    "print(crowd_data[\"fips_code\"].isna().sum(), \"out of\", len(crowd_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05653b42",
   "metadata": {},
   "source": [
    "0 unresolved localities out of 16768; we need not discard any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e94d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crowd_data = crowd_data[crowd_data[\"fips_code\"].notna()]\n",
    "# crowd_data[\"fips_code\"] = crowd_data[\"fips_code\"].astype(int)\n",
    "# crowd_data[\"fips_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dde33",
   "metadata": {},
   "source": [
    "Reindex to requisite columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d7913b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>banking and finance; economy; energy; environm...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>policing</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>education; racism</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>38085.0</td>\n",
       "      <td>protest; occupying land</td>\n",
       "      <td>energy; environment</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>presidency</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32405</th>\n",
       "      <td>2018</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>foreign affairs</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32413</th>\n",
       "      <td>2018</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>policing; racism</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32415</th>\n",
       "      <td>2018</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>housing</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32424</th>\n",
       "      <td>2018</td>\n",
       "      <td>30087.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>women's rights</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32426</th>\n",
       "      <td>2018</td>\n",
       "      <td>39035.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>criminal justice</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16764 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  fips_code                     type  \\\n",
       "2      2017    27053.0     protest; banner drop   \n",
       "8      2017    12086.0            demonstration   \n",
       "13     2017    26161.0                  protest   \n",
       "14     2017    38085.0  protest; occupying land   \n",
       "16     2017    36061.0                  protest   \n",
       "...     ...        ...                      ...   \n",
       "32405  2018    37081.0                  protest   \n",
       "32413  2018     1101.0                  protest   \n",
       "32415  2018    17031.0                  protest   \n",
       "32424  2018    30087.0                  protest   \n",
       "32426  2018    39035.0                  protest   \n",
       "\n",
       "                                                  issues  size_mean  size_cat  \n",
       "2      banking and finance; economy; energy; environm...        2.0         1  \n",
       "8                                               policing       18.0         1  \n",
       "13                                     education; racism      200.0         2  \n",
       "14                                   energy; environment      300.0         2  \n",
       "16                                            presidency        2.0         1  \n",
       "...                                                  ...        ...       ...  \n",
       "32405                                    foreign affairs      150.0         2  \n",
       "32413                                   policing; racism       12.0         1  \n",
       "32415                                            housing       20.0         1  \n",
       "32424                                     women's rights      100.0         2  \n",
       "32426                                   criminal justice       30.0         1  \n",
       "\n",
       "[16764 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_data = crowd_data.reindex(columns=[\"year\",\"fips_code\",\"type\",\"issues\",\"size_mean\",\"size_cat\"])\n",
    "crowd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a849b",
   "metadata": {},
   "source": [
    "Correct mislabeled `type`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef0f6f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['protest', 'banner drop', 'demonstration', 'occupying land',\n",
       "       'strike', 'vigil', 'rally', 'direct action', 'march',\n",
       "       'flag burning', 'block highways', 'sign held on crane',\n",
       "       'counterprotest', 'sit-in', 'standoff', 'dance', 'blockade',\n",
       "       'nonviolent blockade', 'road blockade', 'mass mooning',\n",
       "       'kindness march', 'civil disobedience', 'walk-out',\n",
       "       'human barricade', 'block streets', 'sit-in demonstration',\n",
       "       'peace march', 'protest (armed)', 'hunger strike', 'sick out',\n",
       "       'boycott', 'swim protest', 'die-in', 'boycott classes',\n",
       "       'resignation', 'turn backs to speaker', 'protest resignation',\n",
       "       'die in', 'art projection', 'sing-in', 'skits', 'flash mob',\n",
       "       'flotilla', 'counter protest', 'block interactions',\n",
       "       'light rail stop', 'block highway', 'superhero rally',\n",
       "       'aircraft flyover', 'death train', 'street blockade', 'encampment',\n",
       "       '\"drive\"', 'railroad blockade', 'pray-in', 'picket',\n",
       "       'railway blockade', 'protest hike', 'festival',\n",
       "       'protest (walk-out)', 'walk', 'dinner', 'event', 'mass', 'parade',\n",
       "       'standout', 'protest vandalism', 'silent protest', 'parents',\n",
       "       'support walk-out', 'walk-in',\n",
       "       'postcard writing to public officials', 'science fair',\n",
       "       'mock execution', 'teach in', 'on-campus discussion',\n",
       "       'kayak protest', 'iftar', 'moment of silence', 'protest walk',\n",
       "       'prayer rally', 'self-immolation protest',\n",
       "       'counterprotesting families belong together march', 'convoy',\n",
       "       'kneeling protest', 'procession', 'rent strike', 'hunger-strike',\n",
       "       'rallying', '\"kookout\"', 'protest in trees', 'educational event',\n",
       "       'kneeling during national anthem', 'non-partisan rally', 'demo',\n",
       "       'speeches', 'drive by mar-a-lago', 'prayer vigil',\n",
       "       'press conference', 'grade-strike'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_data_type = crowd_data[\"type\"].str.lower()#.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "crowd_data_type_repl_dict = {\"nat'l\":\"national\",\n",
    "                             \",\":\";\", \"/\":\";\", \" and \":\";\", \"'\":\";\", \":\":\";\",\n",
    "                             \"counter-protest\":\"counterprotest\",\n",
    "                             \"demonstraton\":\"demonstration\", \"demonstation\":\"demonstration\", \"demonstrations\":\"demonstration\", \"demonstarion\":\"demonstration\", \n",
    "                             \"demonstratin\":\"demonstration\", \"deomnstration\":\"demonstration\", \"demonstartion\":\"demonstration\",\n",
    "                             \"flashmob\":\"flash mob\",\n",
    "                             \"marach\":\"march\",\n",
    "                             \"protests\":\"protest\", \"protestors\":\"protest\", \"protesst\":\"protest\", \"protest0\":\"protest\",\n",
    "                             \"raly\":\"rally\", \"rallies\":\"rally\",\n",
    "                             \"walk in\":\"walk-in\",\"walkin\":\"walk-in\",\n",
    "                             \"walk out\":\"walk-out\", \"walkout\":\"walk-out\",\"walkut\":\"walk-out\"\n",
    "                             }\n",
    "\n",
    "for key, value in crowd_data_type_repl_dict.items():\n",
    "    crowd_data_type = crowd_data_type.str.replace(key, value)\n",
    "    \n",
    "crowd_data_type = crowd_data_type.str.split(\";\").explode().str.lstrip().str.rstrip()\n",
    "crowd_data_type = crowd_data_type.astype(str)\n",
    "crowd_data_type = crowd_data_type.loc[(crowd_data_type != \"nan\") & (crowd_data_type != \"m\") & (crowd_data_type != \"0.0\")]\n",
    "crowd_data_type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1257eee",
   "metadata": {},
   "source": [
    "Onehot encode `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41416dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>\"drive\"</th>\n",
       "      <th>\"kookout\"</th>\n",
       "      <th>aircraft flyover</th>\n",
       "      <th>art projection</th>\n",
       "      <th>banner drop</th>\n",
       "      <th>...</th>\n",
       "      <th>strike</th>\n",
       "      <th>superhero rally</th>\n",
       "      <th>support walk-out</th>\n",
       "      <th>swim protest</th>\n",
       "      <th>teach in</th>\n",
       "      <th>turn backs to speaker</th>\n",
       "      <th>vigil</th>\n",
       "      <th>walk</th>\n",
       "      <th>walk-in</th>\n",
       "      <th>walk-out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>banking and finance; economy; energy; environm...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>policing</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>education; racism</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>38085.0</td>\n",
       "      <td>energy; environment</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>presidency</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32405</th>\n",
       "      <td>2018</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>foreign affairs</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32413</th>\n",
       "      <td>2018</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>policing; racism</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32415</th>\n",
       "      <td>2018</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>housing</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32424</th>\n",
       "      <td>2018</td>\n",
       "      <td>30087.0</td>\n",
       "      <td>women's rights</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32426</th>\n",
       "      <td>2018</td>\n",
       "      <td>39035.0</td>\n",
       "      <td>criminal justice</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16764 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  fips_code                                             issues  \\\n",
       "2      2017    27053.0  banking and finance; economy; energy; environm...   \n",
       "8      2017    12086.0                                           policing   \n",
       "13     2017    26161.0                                  education; racism   \n",
       "14     2017    38085.0                                energy; environment   \n",
       "16     2017    36061.0                                         presidency   \n",
       "...     ...        ...                                                ...   \n",
       "32405  2018    37081.0                                    foreign affairs   \n",
       "32413  2018     1101.0                                   policing; racism   \n",
       "32415  2018    17031.0                                            housing   \n",
       "32424  2018    30087.0                                     women's rights   \n",
       "32426  2018    39035.0                                   criminal justice   \n",
       "\n",
       "       size_mean  size_cat \"drive\" \"kookout\" aircraft flyover art projection  \\\n",
       "2            2.0         1   False     False            False          False   \n",
       "8           18.0         1   False     False            False          False   \n",
       "13         200.0         2   False     False            False          False   \n",
       "14         300.0         2   False     False            False          False   \n",
       "16           2.0         1   False     False            False          False   \n",
       "...          ...       ...     ...       ...              ...            ...   \n",
       "32405      150.0         2   False     False            False          False   \n",
       "32413       12.0         1   False     False            False          False   \n",
       "32415       20.0         1   False     False            False          False   \n",
       "32424      100.0         2   False     False            False          False   \n",
       "32426       30.0         1   False     False            False          False   \n",
       "\n",
       "      banner drop  ... strike superhero rally support walk-out swim protest  \\\n",
       "2            True  ...  False           False            False        False   \n",
       "8           False  ...  False           False            False        False   \n",
       "13          False  ...  False           False            False        False   \n",
       "14          False  ...  False           False            False        False   \n",
       "16          False  ...  False           False            False        False   \n",
       "...           ...  ...    ...             ...              ...          ...   \n",
       "32405       False  ...  False           False            False        False   \n",
       "32413       False  ...  False           False            False        False   \n",
       "32415       False  ...  False           False            False        False   \n",
       "32424       False  ...  False           False            False        False   \n",
       "32426       False  ...  False           False            False        False   \n",
       "\n",
       "      teach in turn backs to speaker  vigil   walk walk-in walk-out  \n",
       "2        False                 False  False  False   False    False  \n",
       "8        False                 False  False  False   False    False  \n",
       "13       False                 False  False  False   False    False  \n",
       "14       False                 False  False  False   False    False  \n",
       "16       False                 False  False  False   False    False  \n",
       "...        ...                   ...    ...    ...     ...      ...  \n",
       "32405    False                 False  False  False   False    False  \n",
       "32413    False                 False  False  False   False    False  \n",
       "32415    False                 False  False  False   False    False  \n",
       "32424    False                 False  False  False   False    False  \n",
       "32426    False                 False  False  False   False    False  \n",
       "\n",
       "[16764 rows x 105 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_onehot = pd.get_dummies(crowd_data_type).groupby(crowd_data_type.index).any()\n",
    "type_onehot\n",
    "\n",
    "crowd_data.drop(columns=\"type\").join(type_onehot)\n",
    "\n",
    "\n",
    "# crowd_census_data_issues_exploded = crowd_census_data[\"issues\"].explode()\n",
    "# problems_onehot = pd.get_dummies(crowd_census_data_problems_exploded).groupby(crowd_census_data_problems_exploded.index).any()\n",
    "# problems_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1b497",
   "metadata": {},
   "source": [
    "### American Community Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9290640",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"29e7dfea2f8b253a0a10ccd9626f78e49f4f0a4f\"\n",
    "\n",
    "def regex_filter(string, myregex, inverse=False):\n",
    "    if string:\n",
    "        mo = re.search(myregex, string)\n",
    "        if inverse:\n",
    "            if mo:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            if mo:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "census_vars_orig = ced.variables.search(ACS1_PROFILE, 2017)\n",
    "census_vars = census_vars_orig[(census_vars_orig[\"GROUP\"] != \"N/A\")]\n",
    "census_vars_pcts = census_vars[census_vars[\"VARIABLE\"].apply(regex_filter,myregex=r\"PE$\")]\n",
    "census_vars_pcts_no_ratio = census_vars_pcts[census_vars_pcts[\"LABEL\"].apply(regex_filter,myregex=r\"Sex\\sratio\",inverse=True)]\n",
    "\n",
    "census_vars_agg = census_vars_pcts_no_ratio[census_vars_pcts_no_ratio[\"LABEL\"].apply(regex_filter,myregex=r\"(Median)|(Mean)|(Average)|(Per\\s)|(\\srate)\")]\n",
    "census_vars_agg[[\"VARIABLE\",\"LABEL\"]]\n",
    "\n",
    "census_vars_pcts_no_ratio_agg = census_vars_pcts_no_ratio.drop(census_vars_agg.index)\n",
    "\n",
    "census_vars_agg_ests = census_vars_agg[\"VARIABLE\"].apply(lambda x:x.replace(\"PE\",\"E\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f20fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               NAME\n",
       "1      DP02PR_0001PE\n",
       "2      DP02PR_0002PE\n",
       "3      DP02PR_0003PE\n",
       "4      DP02PR_0004PE\n",
       "           ...      \n",
       "665      DP05_0085PE\n",
       "666       DP05_0086E\n",
       "667      DP05_0087PE\n",
       "668      DP05_0088PE\n",
       "669      DP05_0089PE\n",
       "Length: 670, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_vars_to_drop = [\"DP05_0033PE\", # same as DP05_0001PE (Total Population)\n",
    "                       \"DP05_0086PE\" # total housing units—replace with DP05_0086E\n",
    "\n",
    "                       ]\n",
    "\n",
    "census_vars_to_add = [\"DP05_0086E\"\n",
    "                      ]\n",
    "\n",
    "census_vars_pcts_no_ratio_agg_drop = census_vars_pcts_no_ratio_agg[\"VARIABLE\"][~census_vars_pcts_no_ratio_agg[\"VARIABLE\"].isin(census_vars_to_drop)]\n",
    "\n",
    "census_vars_final = pd.concat([pd.Series([\"NAME\"]), \n",
    "                               pd.concat([census_vars_pcts_no_ratio_agg_drop, \n",
    "                                          census_vars_agg_ests, \n",
    "                                          pd.Series(census_vars_to_add)], \n",
    "                                         ignore_index=True).sort_values()], \n",
    "                              ignore_index=True)\n",
    "\n",
    "census_vars_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bd182",
   "metadata": {},
   "source": [
    "Solely request percents. The list of variables includes what should be the total population as a percent—`DP05_0001PE`—but, as that would always be 100%, the wise people at the US Census Bureau made such equal to `DP05_0001E`, the total population estimate, necessitating no further adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5244f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+ Estimate\n",
       "    + HOUSEHOLDS BY TYPE\n",
       "        + Total households (DP02_0001E)\n",
       "            + Family households (families) (DP02_0002E)\n",
       "                + With own children of the householder under 18 years (DP02_0003E)\n",
       "                + Married-couple family (DP02_0004E)\n",
       "                    + With own children of the householder under 18 years (DP02_0005E)\n",
       "                + Male householder, no wife present, family (DP02_0006E)\n",
       "                    + With own children of the householder under 18 years (DP02_0007E)\n",
       "                + Female householder, no husband present, family (DP02_0008E)\n",
       "                    + With own children of the householder under 18 years (DP02_0009E)\n",
       "            + Nonfamily households (DP02_0010E)\n",
       "                + Householder living alone (DP02_0011E)\n",
       "                    + 65 years and over (DP02_0012E)\n",
       "        + Households with one or more people under 18 years (DP02_0013E)\n",
       "        + Households with one or more people 65 years and over (DP02_0014E)\n",
       "        + Average household size (DP02_0015E)\n",
       "        + Average family size (DP02_0016E)\n",
       "    + RELATIONSHIP\n",
       "        + Population in households (DP02_0017E)\n",
       "            + Householder (DP02_0018E)\n",
       "            + Spouse (DP02_0019E)\n",
       "            + Child (DP02_0020E)\n",
       "            + Other relatives (DP02_0021E)\n",
       "            + Nonrelatives (DP02_0022E)\n",
       "                + Unmarried partner (DP02_0023E)\n",
       "    + MARITAL STATUS\n",
       "        + Males 15 years and over (DP02_0024E)\n",
       "            + Never married (DP02_0025E)\n",
       "            + Now married, except separated (DP02_0026E)\n",
       "            + Separated (DP02_0027E)\n",
       "            + Widowed (DP02_0028E)\n",
       "            + Divorced (DP02_0029E)\n",
       "        + Females 15 years and over (DP02_0030E)\n",
       "            + Never married (DP02_0031E)\n",
       "            + Now married, except separated (DP02_0032E)\n",
       "            + Separated (DP02_0033E)\n",
       "            + Widowed (DP02_0034E)\n",
       "            + Divorced (DP02_0035E)\n",
       "    + FERTILITY\n",
       "        + Number of women 15 to 50 years old who had a birth in the past 12 months (DP02_0036E)\n",
       "            + Unmarried women (widowed, divorced, and never married) (DP02_0037E)\n",
       "                + Per 1,000 unmarried women (DP02_0038E)\n",
       "            + Per 1,000 women 15 to 50 years old (DP02_0039E)\n",
       "            + Per 1,000 women 15 to 19 years old (DP02_0040E)\n",
       "            + Per 1,000 women 20 to 34 years old (DP02_0041E)\n",
       "            + Per 1,000 women 35 to 50 years old (DP02_0042E)\n",
       "    + GRANDPARENTS\n",
       "        + Number of grandparents living with own grandchildren under 18 years (DP02_0043E)\n",
       "            + Grandparents responsible for grandchildren (DP02_0044E)\n",
       "            + Years responsible for grandchildren\n",
       "                + Less than 1 year (DP02_0045E)\n",
       "                + 1 or 2 years (DP02_0046E)\n",
       "                + 3 or 4 years (DP02_0047E)\n",
       "                + 5 or more years (DP02_0048E)\n",
       "        + Number of grandparents responsible for own grandchildren under 18 years (DP02_0049E)\n",
       "            + Who are female (DP02_0050E)\n",
       "            + Who are married (DP02_0051E)\n",
       "    + SCHOOL ENROLLMENT\n",
       "        + Population 3 years and over enrolled in school (DP02_0052E)\n",
       "            + Nursery school, preschool (DP02_0053E)\n",
       "            + Kindergarten (DP02_0054E)\n",
       "            + Elementary school (grades 1-8) (DP02_0055E)\n",
       "            + High school (grades 9-12) (DP02_0056E)\n",
       "            + College or graduate school (DP02_0057E)\n",
       "    + EDUCATIONAL ATTAINMENT\n",
       "        + Population 25 years and over (DP02_0058E)\n",
       "            + Less than 9th grade (DP02_0059E)\n",
       "            + 9th to 12th grade, no diploma (DP02_0060E)\n",
       "            + High school graduate (includes equivalency) (DP02_0061E)\n",
       "            + Some college, no degree (DP02_0062E)\n",
       "            + Associate's degree (DP02_0063E)\n",
       "            + Bachelor's degree (DP02_0064E)\n",
       "            + Graduate or professional degree (DP02_0065E)\n",
       "        + Percent high school graduate or higher (DP02_0066E)\n",
       "        + Percent bachelor's degree or higher (DP02_0067E)\n",
       "    + VETERAN STATUS\n",
       "        + Civilian population 18 years and over (DP02_0068E)\n",
       "            + Civilian veterans (DP02_0069E)\n",
       "    + DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION\n",
       "        + Total Civilian Noninstitutionalized Population (DP02_0070E)\n",
       "            + With a disability (DP02_0071E)\n",
       "        + Under 18 years (DP02_0072E)\n",
       "            + With a disability (DP02_0073E)\n",
       "        + 18 to 64 years (DP02_0074E)\n",
       "            + With a disability (DP02_0075E)\n",
       "        + 65 years and over (DP02_0076E)\n",
       "            + With a disability (DP02_0077E)\n",
       "    + RESIDENCE 1 YEAR AGO\n",
       "        + Population 1 year and over (DP02_0078E)\n",
       "            + Same house (DP02_0079E)\n",
       "            + Different house in the U.S. (DP02_0080E)\n",
       "                + Same county (DP02_0081E)\n",
       "                + Different county (DP02_0082E)\n",
       "                    + Same state (DP02_0083E)\n",
       "                    + Different state (DP02_0084E)\n",
       "            + Abroad (DP02_0085E)\n",
       "    + PLACE OF BIRTH\n",
       "        + Total population (DP02_0086E)\n",
       "            + Native (DP02_0087E)\n",
       "                + Born in United States (DP02_0088E)\n",
       "                    + State of residence (DP02_0089E)\n",
       "                    + Different state (DP02_0090E)\n",
       "                + Born in Puerto Rico, U.S. Island areas, or born abroad to American parent(s) (DP02_0091E)\n",
       "            + Foreign born (DP02_0092E)\n",
       "    + U.S. CITIZENSHIP STATUS\n",
       "        + Foreign-born population (DP02_0093E)\n",
       "            + Naturalized U.S. citizen (DP02_0094E)\n",
       "            + Not a U.S. citizen (DP02_0095E)\n",
       "    + YEAR OF ENTRY\n",
       "        + Population born outside the United States (DP02_0096E)\n",
       "        + Native (DP02_0097E)\n",
       "            + Entered 2010 or later (DP02_0098E)\n",
       "            + Entered before 2010 (DP02_0099E)\n",
       "        + Foreign born (DP02_0100E)\n",
       "            + Entered 2010 or later (DP02_0101E)\n",
       "            + Entered before 2010 (DP02_0102E)\n",
       "    + WORLD REGION OF BIRTH OF FOREIGN BORN\n",
       "        + Foreign-born population, excluding population born at sea (DP02_0103E)\n",
       "            + Europe (DP02_0104E)\n",
       "            + Asia (DP02_0105E)\n",
       "            + Africa (DP02_0106E)\n",
       "            + Oceania (DP02_0107E)\n",
       "            + Latin America (DP02_0108E)\n",
       "            + Northern America (DP02_0109E)\n",
       "    + LANGUAGE SPOKEN AT HOME\n",
       "        + Population 5 years and over (DP02_0110E)\n",
       "            + English only (DP02_0111E)\n",
       "            + Language other than English (DP02_0112E)\n",
       "                + Speak English less than \"very well\" (DP02_0113E)\n",
       "            + Spanish (DP02_0114E)\n",
       "                + Speak English less than \"very well\" (DP02_0115E)\n",
       "            + Other Indo-European languages (DP02_0116E)\n",
       "                + Speak English less than \"very well\" (DP02_0117E)\n",
       "            + Asian and Pacific Islander languages (DP02_0118E)\n",
       "                + Speak English less than \"very well\" (DP02_0119E)\n",
       "            + Other languages (DP02_0120E)\n",
       "                + Speak English less than \"very well\" (DP02_0121E)\n",
       "    + ANCESTRY\n",
       "        + Total population (DP02_0122E)\n",
       "            + American (DP02_0123E)\n",
       "            + Arab (DP02_0124E)\n",
       "            + Czech (DP02_0125E)\n",
       "            + Danish (DP02_0126E)\n",
       "            + Dutch (DP02_0127E)\n",
       "            + English (DP02_0128E)\n",
       "            + French (except Basque) (DP02_0129E)\n",
       "            + French Canadian (DP02_0130E)\n",
       "            + German (DP02_0131E)\n",
       "            + Greek (DP02_0132E)\n",
       "            + Hungarian (DP02_0133E)\n",
       "            + Irish (DP02_0134E)\n",
       "            + Italian (DP02_0135E)\n",
       "            + Lithuanian (DP02_0136E)\n",
       "            + Norwegian (DP02_0137E)\n",
       "            + Polish (DP02_0138E)\n",
       "            + Portuguese (DP02_0139E)\n",
       "            + Russian (DP02_0140E)\n",
       "            + Scotch-Irish (DP02_0141E)\n",
       "            + Scottish (DP02_0142E)\n",
       "            + Slovak (DP02_0143E)\n",
       "            + Subsaharan African (DP02_0144E)\n",
       "            + Swedish (DP02_0145E)\n",
       "            + Swiss (DP02_0146E)\n",
       "            + Ukrainian (DP02_0147E)\n",
       "            + Welsh (DP02_0148E)\n",
       "            + West Indian (excluding Hispanic origin groups) (DP02_0149E)\n",
       "    + COMPUTERS AND INTERNET USE\n",
       "        + Total households (DP02_0150E)\n",
       "            + With a computer (DP02_0151E)\n",
       "            + With a broadband Internet subscription (DP02_0152E)\n",
       "+ Percent\n",
       "    + HOUSEHOLDS BY TYPE\n",
       "        + Total households (DP02_0001PE)\n",
       "            + Family households (families) (DP02_0002PE)\n",
       "                + With own children of the householder under 18 years (DP02_0003PE)\n",
       "                + Married-couple family (DP02_0004PE)\n",
       "                    + With own children of the householder under 18 years (DP02_0005PE)\n",
       "                + Male householder, no wife present, family (DP02_0006PE)\n",
       "                    + With own children of the householder under 18 years (DP02_0007PE)\n",
       "                + Female householder, no husband present, family (DP02_0008PE)\n",
       "                    + With own children of the householder under 18 years (DP02_0009PE)\n",
       "            + Nonfamily households (DP02_0010PE)\n",
       "                + Householder living alone (DP02_0011PE)\n",
       "                    + 65 years and over (DP02_0012PE)\n",
       "        + Households with one or more people under 18 years (DP02_0013PE)\n",
       "        + Households with one or more people 65 years and over (DP02_0014PE)\n",
       "        + Average household size (DP02_0015PE)\n",
       "        + Average family size (DP02_0016PE)\n",
       "    + RELATIONSHIP\n",
       "        + Population in households (DP02_0017PE)\n",
       "            + Householder (DP02_0018PE)\n",
       "            + Spouse (DP02_0019PE)\n",
       "            + Child (DP02_0020PE)\n",
       "            + Other relatives (DP02_0021PE)\n",
       "            + Nonrelatives (DP02_0022PE)\n",
       "                + Unmarried partner (DP02_0023PE)\n",
       "    + MARITAL STATUS\n",
       "        + Males 15 years and over (DP02_0024PE)\n",
       "            + Never married (DP02_0025PE)\n",
       "            + Now married, except separated (DP02_0026PE)\n",
       "            + Separated (DP02_0027PE)\n",
       "            + Widowed (DP02_0028PE)\n",
       "            + Divorced (DP02_0029PE)\n",
       "        + Females 15 years and over (DP02_0030PE)\n",
       "            + Never married (DP02_0031PE)\n",
       "            + Now married, except separated (DP02_0032PE)\n",
       "            + Separated (DP02_0033PE)\n",
       "            + Widowed (DP02_0034PE)\n",
       "            + Divorced (DP02_0035PE)\n",
       "    + FERTILITY\n",
       "        + Number of women 15 to 50 years old who had a birth in the past 12 months (DP02_0036PE)\n",
       "            + Unmarried women (widowed, divorced, and never married) (DP02_0037PE)\n",
       "                + Per 1,000 unmarried women (DP02_0038PE)\n",
       "            + Per 1,000 women 15 to 50 years old (DP02_0039PE)\n",
       "            + Per 1,000 women 15 to 19 years old (DP02_0040PE)\n",
       "            + Per 1,000 women 20 to 34 years old (DP02_0041PE)\n",
       "            + Per 1,000 women 35 to 50 years old (DP02_0042PE)\n",
       "    + GRANDPARENTS\n",
       "        + Number of grandparents living with own grandchildren under 18 years (DP02_0043PE)\n",
       "            + Grandparents responsible for grandchildren (DP02_0044PE)\n",
       "            + Years responsible for grandchildren\n",
       "                + Less than 1 year (DP02_0045PE)\n",
       "                + 1 or 2 years (DP02_0046PE)\n",
       "                + 3 or 4 years (DP02_0047PE)\n",
       "                + 5 or more years (DP02_0048PE)\n",
       "        + Number of grandparents responsible for own grandchildren under 18 years (DP02_0049PE)\n",
       "            + Who are female (DP02_0050PE)\n",
       "            + Who are married (DP02_0051PE)\n",
       "    + SCHOOL ENROLLMENT\n",
       "        + Population 3 years and over enrolled in school (DP02_0052PE)\n",
       "            + Nursery school, preschool (DP02_0053PE)\n",
       "            + Kindergarten (DP02_0054PE)\n",
       "            + Elementary school (grades 1-8) (DP02_0055PE)\n",
       "            + High school (grades 9-12) (DP02_0056PE)\n",
       "            + College or graduate school (DP02_0057PE)\n",
       "    + EDUCATIONAL ATTAINMENT\n",
       "        + Population 25 years and over (DP02_0058PE)\n",
       "            + Less than 9th grade (DP02_0059PE)\n",
       "            + 9th to 12th grade, no diploma (DP02_0060PE)\n",
       "            + High school graduate (includes equivalency) (DP02_0061PE)\n",
       "            + Some college, no degree (DP02_0062PE)\n",
       "            + Associate's degree (DP02_0063PE)\n",
       "            + Bachelor's degree (DP02_0064PE)\n",
       "            + Graduate or professional degree (DP02_0065PE)\n",
       "        + Percent high school graduate or higher (DP02_0066PE)\n",
       "        + Percent bachelor's degree or higher (DP02_0067PE)\n",
       "    + VETERAN STATUS\n",
       "        + Civilian population 18 years and over (DP02_0068PE)\n",
       "            + Civilian veterans (DP02_0069PE)\n",
       "    + DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION\n",
       "        + Total Civilian Noninstitutionalized Population (DP02_0070PE)\n",
       "            + With a disability (DP02_0071PE)\n",
       "        + Under 18 years (DP02_0072PE)\n",
       "            + With a disability (DP02_0073PE)\n",
       "        + 18 to 64 years (DP02_0074PE)\n",
       "            + With a disability (DP02_0075PE)\n",
       "        + 65 years and over (DP02_0076PE)\n",
       "            + With a disability (DP02_0077PE)\n",
       "    + RESIDENCE 1 YEAR AGO\n",
       "        + Population 1 year and over (DP02_0078PE)\n",
       "            + Same house (DP02_0079PE)\n",
       "            + Different house in the U.S. (DP02_0080PE)\n",
       "                + Same county (DP02_0081PE)\n",
       "                + Different county (DP02_0082PE)\n",
       "                    + Same state (DP02_0083PE)\n",
       "                    + Different state (DP02_0084PE)\n",
       "            + Abroad (DP02_0085PE)\n",
       "    + PLACE OF BIRTH\n",
       "        + Total population (DP02_0086PE)\n",
       "            + Native (DP02_0087PE)\n",
       "                + Born in United States (DP02_0088PE)\n",
       "                    + State of residence (DP02_0089PE)\n",
       "                    + Different state (DP02_0090PE)\n",
       "                + Born in Puerto Rico, U.S. Island areas, or born abroad to American parent(s) (DP02_0091PE)\n",
       "            + Foreign born (DP02_0092PE)\n",
       "    + U.S. CITIZENSHIP STATUS\n",
       "        + Foreign-born population (DP02_0093PE)\n",
       "            + Naturalized U.S. citizen (DP02_0094PE)\n",
       "            + Not a U.S. citizen (DP02_0095PE)\n",
       "    + YEAR OF ENTRY\n",
       "        + Population born outside the United States (DP02_0096PE)\n",
       "        + Native (DP02_0097PE)\n",
       "            + Entered 2010 or later (DP02_0098PE)\n",
       "            + Entered before 2010 (DP02_0099PE)\n",
       "        + Foreign born (DP02_0100PE)\n",
       "            + Entered 2010 or later (DP02_0101PE)\n",
       "            + Entered before 2010 (DP02_0102PE)\n",
       "    + WORLD REGION OF BIRTH OF FOREIGN BORN\n",
       "        + Foreign-born population, excluding population born at sea (DP02_0103PE)\n",
       "            + Europe (DP02_0104PE)\n",
       "            + Asia (DP02_0105PE)\n",
       "            + Africa (DP02_0106PE)\n",
       "            + Oceania (DP02_0107PE)\n",
       "            + Latin America (DP02_0108PE)\n",
       "            + Northern America (DP02_0109PE)\n",
       "    + LANGUAGE SPOKEN AT HOME\n",
       "        + Population 5 years and over (DP02_0110PE)\n",
       "            + English only (DP02_0111PE)\n",
       "            + Language other than English (DP02_0112PE)\n",
       "                + Speak English less than \"very well\" (DP02_0113PE)\n",
       "            + Spanish (DP02_0114PE)\n",
       "                + Speak English less than \"very well\" (DP02_0115PE)\n",
       "            + Other Indo-European languages (DP02_0116PE)\n",
       "                + Speak English less than \"very well\" (DP02_0117PE)\n",
       "            + Asian and Pacific Islander languages (DP02_0118PE)\n",
       "                + Speak English less than \"very well\" (DP02_0119PE)\n",
       "            + Other languages (DP02_0120PE)\n",
       "                + Speak English less than \"very well\" (DP02_0121PE)\n",
       "    + ANCESTRY\n",
       "        + Total population (DP02_0122PE)\n",
       "            + American (DP02_0123PE)\n",
       "            + Arab (DP02_0124PE)\n",
       "            + Czech (DP02_0125PE)\n",
       "            + Danish (DP02_0126PE)\n",
       "            + Dutch (DP02_0127PE)\n",
       "            + English (DP02_0128PE)\n",
       "            + French (except Basque) (DP02_0129PE)\n",
       "            + French Canadian (DP02_0130PE)\n",
       "            + German (DP02_0131PE)\n",
       "            + Greek (DP02_0132PE)\n",
       "            + Hungarian (DP02_0133PE)\n",
       "            + Irish (DP02_0134PE)\n",
       "            + Italian (DP02_0135PE)\n",
       "            + Lithuanian (DP02_0136PE)\n",
       "            + Norwegian (DP02_0137PE)\n",
       "            + Polish (DP02_0138PE)\n",
       "            + Portuguese (DP02_0139PE)\n",
       "            + Russian (DP02_0140PE)\n",
       "            + Scotch-Irish (DP02_0141PE)\n",
       "            + Scottish (DP02_0142PE)\n",
       "            + Slovak (DP02_0143PE)\n",
       "            + Subsaharan African (DP02_0144PE)\n",
       "            + Swedish (DP02_0145PE)\n",
       "            + Swiss (DP02_0146PE)\n",
       "            + Ukrainian (DP02_0147PE)\n",
       "            + Welsh (DP02_0148PE)\n",
       "            + West Indian (excluding Hispanic origin groups) (DP02_0149PE)\n",
       "    + COMPUTERS AND INTERNET USE\n",
       "        + Total households (DP02_0150PE)\n",
       "            + With a computer (DP02_0151PE)\n",
       "            + With a broadband Internet subscription (DP02_0152PE)\n",
       "+ Percent Margin of Error\n",
       "    + HOUSEHOLDS BY TYPE\n",
       "        + Total households (DP02_0001PM)\n",
       "            + Family households (families) (DP02_0002PM)\n",
       "                + With own children of the householder under 18 years (DP02_0003PM)\n",
       "                + Married-couple family (DP02_0004PM)\n",
       "                    + With own children of the householder under 18 years (DP02_0005PM)\n",
       "                + Male householder, no wife present, family (DP02_0006PM)\n",
       "                    + With own children of the householder under 18 years (DP02_0007PM)\n",
       "                + Female householder, no husband present, family (DP02_0008PM)\n",
       "                    + With own children of the householder under 18 years (DP02_0009PM)\n",
       "            + Nonfamily households (DP02_0010PM)\n",
       "                + Householder living alone (DP02_0011PM)\n",
       "                    + 65 years and over (DP02_0012PM)\n",
       "        + Households with one or more people under 18 years (DP02_0013PM)\n",
       "        + Households with one or more people 65 years and over (DP02_0014PM)\n",
       "        + Average household size (DP02_0015PM)\n",
       "        + Average family size (DP02_0016PM)\n",
       "    + RELATIONSHIP\n",
       "        + Population in households (DP02_0017PM)\n",
       "            + Householder (DP02_0018PM)\n",
       "            + Spouse (DP02_0019PM)\n",
       "            + Child (DP02_0020PM)\n",
       "            + Other relatives (DP02_0021PM)\n",
       "            + Nonrelatives (DP02_0022PM)\n",
       "                + Unmarried partner (DP02_0023PM)\n",
       "    + MARITAL STATUS\n",
       "        + Males 15 years and over (DP02_0024PM)\n",
       "            + Never married (DP02_0025PM)\n",
       "            + Now married, except separated (DP02_0026PM)\n",
       "            + Separated (DP02_0027PM)\n",
       "            + Widowed (DP02_0028PM)\n",
       "            + Divorced (DP02_0029PM)\n",
       "        + Females 15 years and over (DP02_0030PM)\n",
       "            + Never married (DP02_0031PM)\n",
       "            + Now married, except separated (DP02_0032PM)\n",
       "            + Separated (DP02_0033PM)\n",
       "            + Widowed (DP02_0034PM)\n",
       "            + Divorced (DP02_0035PM)\n",
       "    + FERTILITY\n",
       "        + Number of women 15 to 50 years old who had a birth in the past 12 months (DP02_0036PM)\n",
       "            + Unmarried women (widowed, divorced, and never married) (DP02_0037PM)\n",
       "                + Per 1,000 unmarried women (DP02_0038PM)\n",
       "            + Per 1,000 women 15 to 50 years old (DP02_0039PM)\n",
       "            + Per 1,000 women 15 to 19 years old (DP02_0040PM)\n",
       "            + Per 1,000 women 20 to 34 years old (DP02_0041PM)\n",
       "            + Per 1,000 women 35 to 50 years old (DP02_0042PM)\n",
       "    + GRANDPARENTS\n",
       "        + Number of grandparents living with own grandchildren under 18 years (DP02_0043PM)\n",
       "            + Grandparents responsible for grandchildren (DP02_0044PM)\n",
       "            + Years responsible for grandchildren\n",
       "                + Less than 1 year (DP02_0045PM)\n",
       "                + 1 or 2 years (DP02_0046PM)\n",
       "                + 3 or 4 years (DP02_0047PM)\n",
       "                + 5 or more years (DP02_0048PM)\n",
       "        + Number of grandparents responsible for own grandchildren under 18 years (DP02_0049PM)\n",
       "            + Who are female (DP02_0050PM)\n",
       "            + Who are married (DP02_0051PM)\n",
       "    + SCHOOL ENROLLMENT\n",
       "        + Population 3 years and over enrolled in school (DP02_0052PM)\n",
       "            + Nursery school, preschool (DP02_0053PM)\n",
       "            + Kindergarten (DP02_0054PM)\n",
       "            + Elementary school (grades 1-8) (DP02_0055PM)\n",
       "            + High school (grades 9-12) (DP02_0056PM)\n",
       "            + College or graduate school (DP02_0057PM)\n",
       "    + EDUCATIONAL ATTAINMENT\n",
       "        + Population 25 years and over (DP02_0058PM)\n",
       "            + Less than 9th grade (DP02_0059PM)\n",
       "            + 9th to 12th grade, no diploma (DP02_0060PM)\n",
       "            + High school graduate (includes equivalency) (DP02_0061PM)\n",
       "            + Some college, no degree (DP02_0062PM)\n",
       "            + Associate's degree (DP02_0063PM)\n",
       "            + Bachelor's degree (DP02_0064PM)\n",
       "            + Graduate or professional degree (DP02_0065PM)\n",
       "        + Percent high school graduate or higher (DP02_0066PM)\n",
       "        + Percent bachelor's degree or higher (DP02_0067PM)\n",
       "    + VETERAN STATUS\n",
       "        + Civilian population 18 years and over (DP02_0068PM)\n",
       "            + Civilian veterans (DP02_0069PM)\n",
       "    + DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION\n",
       "        + Total Civilian Noninstitutionalized Population (DP02_0070PM)\n",
       "            + With a disability (DP02_0071PM)\n",
       "        + Under 18 years (DP02_0072PM)\n",
       "            + With a disability (DP02_0073PM)\n",
       "        + 18 to 64 years (DP02_0074PM)\n",
       "            + With a disability (DP02_0075PM)\n",
       "        + 65 years and over (DP02_0076PM)\n",
       "            + With a disability (DP02_0077PM)\n",
       "    + RESIDENCE 1 YEAR AGO\n",
       "        + Population 1 year and over (DP02_0078PM)\n",
       "            + Same house (DP02_0079PM)\n",
       "            + Different house in the U.S. (DP02_0080PM)\n",
       "                + Same county (DP02_0081PM)\n",
       "                + Different county (DP02_0082PM)\n",
       "                    + Same state (DP02_0083PM)\n",
       "                    + Different state (DP02_0084PM)\n",
       "            + Abroad (DP02_0085PM)\n",
       "    + PLACE OF BIRTH\n",
       "        + Total population (DP02_0086PM)\n",
       "            + Native (DP02_0087PM)\n",
       "                + Born in United States (DP02_0088PM)\n",
       "                    + State of residence (DP02_0089PM)\n",
       "                    + Different state (DP02_0090PM)\n",
       "                + Born in Puerto Rico, U.S. Island areas, or born abroad to American parent(s) (DP02_0091PM)\n",
       "            + Foreign born (DP02_0092PM)\n",
       "    + U.S. CITIZENSHIP STATUS\n",
       "        + Foreign-born population (DP02_0093PM)\n",
       "            + Naturalized U.S. citizen (DP02_0094PM)\n",
       "            + Not a U.S. citizen (DP02_0095PM)\n",
       "    + YEAR OF ENTRY\n",
       "        + Population born outside the United States (DP02_0096PM)\n",
       "        + Native (DP02_0097PM)\n",
       "            + Entered 2010 or later (DP02_0098PM)\n",
       "            + Entered before 2010 (DP02_0099PM)\n",
       "        + Foreign born (DP02_0100PM)\n",
       "            + Entered 2010 or later (DP02_0101PM)\n",
       "            + Entered before 2010 (DP02_0102PM)\n",
       "    + WORLD REGION OF BIRTH OF FOREIGN BORN\n",
       "        + Foreign-born population, excluding population born at sea (DP02_0103PM)\n",
       "            + Europe (DP02_0104PM)\n",
       "            + Asia (DP02_0105PM)\n",
       "            + Africa (DP02_0106PM)\n",
       "            + Oceania (DP02_0107PM)\n",
       "            + Latin America (DP02_0108PM)\n",
       "            + Northern America (DP02_0109PM)\n",
       "    + LANGUAGE SPOKEN AT HOME\n",
       "        + Population 5 years and over (DP02_0110PM)\n",
       "            + English only (DP02_0111PM)\n",
       "            + Language other than English (DP02_0112PM)\n",
       "                + Speak English less than \"very well\" (DP02_0113PM)\n",
       "            + Spanish (DP02_0114PM)\n",
       "                + Speak English less than \"very well\" (DP02_0115PM)\n",
       "            + Other Indo-European languages (DP02_0116PM)\n",
       "                + Speak English less than \"very well\" (DP02_0117PM)\n",
       "            + Asian and Pacific Islander languages (DP02_0118PM)\n",
       "                + Speak English less than \"very well\" (DP02_0119PM)\n",
       "            + Other languages (DP02_0120PM)\n",
       "                + Speak English less than \"very well\" (DP02_0121PM)\n",
       "    + ANCESTRY\n",
       "        + Total population (DP02_0122PM)\n",
       "            + American (DP02_0123PM)\n",
       "            + Arab (DP02_0124PM)\n",
       "            + Czech (DP02_0125PM)\n",
       "            + Danish (DP02_0126PM)\n",
       "            + Dutch (DP02_0127PM)\n",
       "            + English (DP02_0128PM)\n",
       "            + French (except Basque) (DP02_0129PM)\n",
       "            + French Canadian (DP02_0130PM)\n",
       "            + German (DP02_0131PM)\n",
       "            + Greek (DP02_0132PM)\n",
       "            + Hungarian (DP02_0133PM)\n",
       "            + Irish (DP02_0134PM)\n",
       "            + Italian (DP02_0135PM)\n",
       "            + Lithuanian (DP02_0136PM)\n",
       "            + Norwegian (DP02_0137PM)\n",
       "            + Polish (DP02_0138PM)\n",
       "            + Portuguese (DP02_0139PM)\n",
       "            + Russian (DP02_0140PM)\n",
       "            + Scotch-Irish (DP02_0141PM)\n",
       "            + Scottish (DP02_0142PM)\n",
       "            + Slovak (DP02_0143PM)\n",
       "            + Subsaharan African (DP02_0144PM)\n",
       "            + Swedish (DP02_0145PM)\n",
       "            + Swiss (DP02_0146PM)\n",
       "            + Ukrainian (DP02_0147PM)\n",
       "            + Welsh (DP02_0148PM)\n",
       "            + West Indian (excluding Hispanic origin groups) (DP02_0149PM)\n",
       "    + COMPUTERS AND INTERNET USE\n",
       "        + Total households (DP02_0150PM)\n",
       "            + With a computer (DP02_0151PM)\n",
       "            + With a broadband Internet subscription (DP02_0152PM)\n",
       "+ Geography (GEO_ID)\n",
       "+ Geographic Area Name (NAME)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ced.variables.group_tree(ACS1_PROFILE, 2017,\"DP02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c899b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>DP02PR_0001PE</th>\n",
       "      <th>DP02PR_0002PE</th>\n",
       "      <th>DP02PR_0003PE</th>\n",
       "      <th>DP02PR_0004PE</th>\n",
       "      <th>DP02PR_0005PE</th>\n",
       "      <th>DP02PR_0006PE</th>\n",
       "      <th>DP02PR_0007PE</th>\n",
       "      <th>DP02PR_0008PE</th>\n",
       "      <th>DP02PR_0009PE</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0080PE</th>\n",
       "      <th>DP05_0081PE</th>\n",
       "      <th>DP05_0082PE</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STATISTICAL_AREA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017</th>\n",
       "      <th>44220</th>\n",
       "      <td>Springfield, OH Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>61265</td>\n",
       "      <td>103123</td>\n",
       "      <td>47.3</td>\n",
       "      <td>52.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44300</th>\n",
       "      <td>State College, PA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66733</td>\n",
       "      <td>129054</td>\n",
       "      <td>53.3</td>\n",
       "      <td>46.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44340</th>\n",
       "      <td>Statesboro, GA Micro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>31072</td>\n",
       "      <td>59095</td>\n",
       "      <td>48.5</td>\n",
       "      <td>51.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44420</th>\n",
       "      <td>Staunton-Waynesboro, VA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>53875</td>\n",
       "      <td>95444</td>\n",
       "      <td>47.7</td>\n",
       "      <td>52.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44620</th>\n",
       "      <td>Stevens Point, WI Micro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>30935</td>\n",
       "      <td>55713</td>\n",
       "      <td>49.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018</th>\n",
       "      <th>31820</th>\n",
       "      <td>Manitowoc, WI Micro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>37578</td>\n",
       "      <td>61915</td>\n",
       "      <td>49.7</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31860</th>\n",
       "      <td>Mankato-North Mankato, MN Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>42424</td>\n",
       "      <td>78514</td>\n",
       "      <td>50.7</td>\n",
       "      <td>49.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31900</th>\n",
       "      <td>Mansfield, OH Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>54173</td>\n",
       "      <td>94217</td>\n",
       "      <td>50.5</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31940</th>\n",
       "      <td>Marinette, WI-MI Micro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45232</td>\n",
       "      <td>51069</td>\n",
       "      <td>50.3</td>\n",
       "      <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>Marion, IN Micro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30494</td>\n",
       "      <td>51938</td>\n",
       "      <td>46.9</td>\n",
       "      <td>53.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1037 rows × 670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         NAME  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                                         \n",
       "2017 44220                                                         Springfield, OH Metro Area   \n",
       "     44300                                                       State College, PA Metro Area   \n",
       "     44340                                                          Statesboro, GA Micro Area   \n",
       "     44420                                                 Staunton-Waynesboro, VA Metro Area   \n",
       "     44620                                                       Stevens Point, WI Micro Area   \n",
       "...                                                                                       ...   \n",
       "2018 31820                                                           Manitowoc, WI Micro Area   \n",
       "     31860                                               Mankato-North Mankato, MN Metro Area   \n",
       "     31900                                                           Mansfield, OH Metro Area   \n",
       "     31940                                                        Marinette, WI-MI Micro Area   \n",
       "     31980                                                              Marion, IN Micro Area   \n",
       "\n",
       "                                                         DP02PR_0001PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0002PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0003PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0004PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0005PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0006PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0007PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0008PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                  \n",
       "2017 44220                                                         NaN   \n",
       "     44300                                                         NaN   \n",
       "     44340                                                         NaN   \n",
       "     44420                                                         NaN   \n",
       "     44620                                                         NaN   \n",
       "...                                                                ...   \n",
       "2018 31820                                                         NaN   \n",
       "     31860                                                         NaN   \n",
       "     31900                                                         NaN   \n",
       "     31940                                                         NaN   \n",
       "     31980                                                         NaN   \n",
       "\n",
       "                                                         DP02PR_0009PE  ...  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                 ...   \n",
       "2017 44220                                                         NaN  ...   \n",
       "     44300                                                         NaN  ...   \n",
       "     44340                                                         NaN  ...   \n",
       "     44420                                                         NaN  ...   \n",
       "     44620                                                         NaN  ...   \n",
       "...                                                                ...  ...   \n",
       "2018 31820                                                         NaN  ...   \n",
       "     31860                                                         NaN  ...   \n",
       "     31900                                                         NaN  ...   \n",
       "     31940                                                         NaN  ...   \n",
       "     31980                                                         NaN  ...   \n",
       "\n",
       "                                                         DP05_0080PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       0.7   \n",
       "     44300                                                       6.8   \n",
       "     44340                                                       1.4   \n",
       "     44420                                                       0.6   \n",
       "     44620                                                       3.3   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       2.8   \n",
       "     31860                                                       1.9   \n",
       "     31900                                                       0.9   \n",
       "     31940                                                       0.3   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0081PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       0.0   \n",
       "     44300                                                       0.0   \n",
       "     44340                                                       0.0   \n",
       "     44420                                                       0.0   \n",
       "     44620                                                       0.0   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       0.0   \n",
       "     31860                                                       0.1   \n",
       "     31900                                                       0.0   \n",
       "     31940                                                       0.0   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0082PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       0.2   \n",
       "     44300                                                       0.0   \n",
       "     44340                                                       0.2   \n",
       "     44420                                                       0.1   \n",
       "     44620                                                       0.0   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       0.0   \n",
       "     31860                                                       0.0   \n",
       "     31900                                                       0.2   \n",
       "     31940                                                       0.0   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0083PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       3.5   \n",
       "     44300                                                       1.2   \n",
       "     44340                                                       2.8   \n",
       "     44420                                                       1.1   \n",
       "     44620                                                       1.3   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       1.1   \n",
       "     31860                                                       2.4   \n",
       "     31900                                                       2.9   \n",
       "     31940                                                       2.0   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0084PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       0.0   \n",
       "     44300                                                       0.2   \n",
       "     44340                                                       0.0   \n",
       "     44420                                                       0.0   \n",
       "     44620                                                       0.1   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       0.0   \n",
       "     31860                                                       0.0   \n",
       "     31900                                                       0.0   \n",
       "     31940                                                       0.0   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0085PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                       3.5   \n",
       "     44300                                                       1.0   \n",
       "     44340                                                       2.8   \n",
       "     44420                                                       1.1   \n",
       "     44620                                                       1.2   \n",
       "...                                                              ...   \n",
       "2018 31820                                                       1.1   \n",
       "     31860                                                       2.4   \n",
       "     31900                                                       2.9   \n",
       "     31940                                                       2.0   \n",
       "     31980                                                       NaN   \n",
       "\n",
       "                                                         DP05_0086E  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...               \n",
       "2017 44220                                                    61265   \n",
       "     44300                                                    66733   \n",
       "     44340                                                    31072   \n",
       "     44420                                                    53875   \n",
       "     44620                                                    30935   \n",
       "...                                                             ...   \n",
       "2018 31820                                                    37578   \n",
       "     31860                                                    42424   \n",
       "     31900                                                    54173   \n",
       "     31940                                                    45232   \n",
       "     31980                                                    30494   \n",
       "\n",
       "                                                         DP05_0087PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                    103123   \n",
       "     44300                                                    129054   \n",
       "     44340                                                     59095   \n",
       "     44420                                                     95444   \n",
       "     44620                                                     55713   \n",
       "...                                                              ...   \n",
       "2018 31820                                                     61915   \n",
       "     31860                                                     78514   \n",
       "     31900                                                     94217   \n",
       "     31940                                                     51069   \n",
       "     31980                                                     51938   \n",
       "\n",
       "                                                         DP05_0088PE  \\\n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...                \n",
       "2017 44220                                                      47.3   \n",
       "     44300                                                      53.3   \n",
       "     44340                                                      48.5   \n",
       "     44420                                                      47.7   \n",
       "     44620                                                      49.5   \n",
       "...                                                              ...   \n",
       "2018 31820                                                      49.7   \n",
       "     31860                                                      50.7   \n",
       "     31900                                                      50.5   \n",
       "     31940                                                      50.3   \n",
       "     31980                                                      46.9   \n",
       "\n",
       "                                                         DP05_0089PE  \n",
       "year METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STAT...               \n",
       "2017 44220                                                      52.7  \n",
       "     44300                                                      46.7  \n",
       "     44340                                                      51.5  \n",
       "     44420                                                      52.3  \n",
       "     44620                                                      50.5  \n",
       "...                                                              ...  \n",
       "2018 31820                                                      50.3  \n",
       "     31860                                                      49.3  \n",
       "     31900                                                      49.5  \n",
       "     31940                                                      49.7  \n",
       "     31980                                                      53.1  \n",
       "\n",
       "[1037 rows x 670 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_data = [ced.download(ACS1_PROFILE, year, census_vars_final, \n",
    "                           metropolitan_statistical_area_micropolitan_statistical_area =\"*\", \n",
    "                           api_key=api_key\n",
    "                           ).set_index(\"METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STATISTICAL_AREA\")\n",
    "               for year in range(2017,2019)]\n",
    "\n",
    "\n",
    "census_data = pd.concat(census_data, keys = np.arange(2017,2019), names=[\"year\"])\n",
    "census_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2edb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DP02PR_0114PE', 'DP02PR_0115PE', 'DP02PR_0116PE', 'DP02PR_0117PE',\n",
       "       'DP02PR_0118PE', 'DP02PR_0119PE', 'DP02PR_0120PE', 'DP02PR_0121PE',\n",
       "       'DP02PR_0122PE', 'DP02PR_0123PE', 'DP02PR_0124PE', 'DP02PR_0125PE',\n",
       "       'DP02PR_0126PE', 'DP02PR_0127PE', 'DP02PR_0128PE', 'DP02PR_0129PE',\n",
       "       'DP02PR_0130PE', 'DP02PR_0131PE', 'DP02PR_0132PE', 'DP02PR_0133PE',\n",
       "       'DP02PR_0134PE', 'DP02PR_0135PE', 'DP02PR_0136PE', 'DP02PR_0137PE',\n",
       "       'DP02PR_0138PE', 'DP02PR_0139PE', 'DP02PR_0140PE', 'DP02PR_0141PE',\n",
       "       'DP02PR_0142PE', 'DP02PR_0143PE', 'DP02PR_0144PE', 'DP02PR_0145PE',\n",
       "       'DP02PR_0146PE', 'DP02PR_0147PE', 'DP02PR_0148PE', 'DP02PR_0149PE',\n",
       "       'DP04_0116PE', 'DP04_0125PE', 'DP04_0135PE', 'DP04_0143PE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_data.columns[census_data.isna().sum() == 1037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metro_micro_area_text = re.compile(r\"(\\s(Metro|Micro)\\sArea)$\")\n",
    "# census_data[\"NAME\"].apply(lambda x: metro_micro_area_text.sub(\"\",x))\n",
    "# # re.sub(string=\"New York Metro Area\", pattern=r\"(Metro|Micro)\\sArea$\", repl=\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32718482",
   "metadata": {},
   "source": [
    "### Most Important Problem Dataset, Second Release (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eddca5",
   "metadata": {},
   "source": [
    "The aggregated data may be found here: https://williamslaro.github.io/talks/dataset2. Previously, the individual data was used, necessitating much preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54b9d9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>numsurveys</th>\n",
       "      <th>numrespondents</th>\n",
       "      <th>perc</th>\n",
       "      <th>catname</th>\n",
       "      <th>catdesc</th>\n",
       "      <th>problemname</th>\n",
       "      <th>problemdesc</th>\n",
       "      <th>problemissues</th>\n",
       "      <th>problemexamples</th>\n",
       "      <th>subname</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>problem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">1939</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>45.401001</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>6.861000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>4.629000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Foreign Policy</td>\n",
       "      <td>includes all mentions of foreign policy and ot...</td>\n",
       "      <td>Foreign policy, national security, nuclear wea...</td>\n",
       "      <td>Foreign affairs, international problems, forei...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>International Economic Relations</td>\n",
       "      <td>includes all mentions of international economi...</td>\n",
       "      <td>Globalization, trade, international cooperatio...</td>\n",
       "      <td>Foreign ownership, foreign competition, trade ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Environment</td>\n",
       "      <td>includes all mentions of the environment, natu...</td>\n",
       "      <td>Environment (general), climate change, polluti...</td>\n",
       "      <td>Environmental protection, conservation, climat...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.432000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Morality</td>\n",
       "      <td>includes all mentions of morals, values, and r...</td>\n",
       "      <td>Morals and values, family values, religion, cu...</td>\n",
       "      <td>Morals, values, ethics, honesty, family breakd...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>4.123000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Politics</td>\n",
       "      <td>includes all mentions of political leaders, va...</td>\n",
       "      <td>Politics, corruption, trustworthiness, leaders...</td>\n",
       "      <td>Politics as usual, abuse of power, greedy poli...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Youth Issues</td>\n",
       "      <td>includes all mentions of youth issues</td>\n",
       "      <td>Youths (general), crime, gangs, school violenc...</td>\n",
       "      <td>Children, morals for youths, juvenile delinque...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Groups</td>\n",
       "      <td>includes all mentions of minority groups</td>\n",
       "      <td>Native Americans, hispanics, Asian Americans, ...</td>\n",
       "      <td>Minorities, American Indians, anti-semitism, M...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>1.461000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Other and All</td>\n",
       "      <td>includes mentions of other problems or all pro...</td>\n",
       "      <td>Other, all</td>\n",
       "      <td>Other, stress, stem cell research, computers a...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>includes mentions of no problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing in particular, no other mention, no pr...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8989</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Don't Know/Refused</td>\n",
       "      <td>includes any mention that is related to don't ...</td>\n",
       "      <td>Don't know, no opinion, not applicable, refusal</td>\n",
       "      <td>Not sure, can't say, not interested, unknown, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1939-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1940</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2907</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1940-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2907</td>\n",
       "      <td>3.474000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>All</td>\n",
       "      <td>1940-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2907</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1940-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2907</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>All</td>\n",
       "      <td>1940-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2907</td>\n",
       "      <td>13.152000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>All</td>\n",
       "      <td>1940-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cat  numsurveys  numrespondents       perc         catname  \\\n",
       "year problem                                                               \n",
       "1939 1          1           3            8989  45.401001  MIPD (General)   \n",
       "     2          1           3            8989   6.861000  MIPD (General)   \n",
       "     3          1           3            8989   0.200000  MIPD (General)   \n",
       "     4          1           3            8989   0.554000  MIPD (General)   \n",
       "     5          1           3            8989   4.629000  MIPD (General)   \n",
       "     6          1           3            8989  33.779999  MIPD (General)   \n",
       "     7          1           3            8989   0.056000  MIPD (General)   \n",
       "     8          1           3            8989   0.044000  MIPD (General)   \n",
       "     9          1           3            8989   1.432000  MIPD (General)   \n",
       "     10         1           3            8989   4.123000  MIPD (General)   \n",
       "     11         1           3            8989   0.700000  MIPD (General)   \n",
       "     12         1           3            8989   0.168000  MIPD (General)   \n",
       "     13         1           3            8989   1.461000  MIPD (General)   \n",
       "     14         1           3            8989   0.078000  MIPD (General)   \n",
       "     15         1           3            8989   0.512000  MIPD (General)   \n",
       "1940 1          1           1            2907  24.200001  MIPD (General)   \n",
       "     2          1           1            2907   3.474000  MIPD (General)   \n",
       "     3          1           1            2907   0.069000  MIPD (General)   \n",
       "     4          1           1            2907   0.172000  MIPD (General)   \n",
       "     5          1           1            2907  13.152000  MIPD (General)   \n",
       "\n",
       "                                             catdesc  \\\n",
       "year problem                                           \n",
       "1939 1        general problem categories in the MIPD   \n",
       "     2        general problem categories in the MIPD   \n",
       "     3        general problem categories in the MIPD   \n",
       "     4        general problem categories in the MIPD   \n",
       "     5        general problem categories in the MIPD   \n",
       "     6        general problem categories in the MIPD   \n",
       "     7        general problem categories in the MIPD   \n",
       "     8        general problem categories in the MIPD   \n",
       "     9        general problem categories in the MIPD   \n",
       "     10       general problem categories in the MIPD   \n",
       "     11       general problem categories in the MIPD   \n",
       "     12       general problem categories in the MIPD   \n",
       "     13       general problem categories in the MIPD   \n",
       "     14       general problem categories in the MIPD   \n",
       "     15       general problem categories in the MIPD   \n",
       "1940 1        general problem categories in the MIPD   \n",
       "     2        general problem categories in the MIPD   \n",
       "     3        general problem categories in the MIPD   \n",
       "     4        general problem categories in the MIPD   \n",
       "     5        general problem categories in the MIPD   \n",
       "\n",
       "                                   problemname  \\\n",
       "year problem                                     \n",
       "1939 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "     6                          Foreign Policy   \n",
       "     7        International Economic Relations   \n",
       "     8                             Environment   \n",
       "     9                                Morality   \n",
       "     10                               Politics   \n",
       "     11                           Youth Issues   \n",
       "     12                                 Groups   \n",
       "     13                          Other and All   \n",
       "     14                                    NaN   \n",
       "     15                     Don't Know/Refused   \n",
       "1940 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "\n",
       "                                                    problemdesc  \\\n",
       "year problem                                                      \n",
       "1939 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "     6        includes all mentions of foreign policy and ot...   \n",
       "     7        includes all mentions of international economi...   \n",
       "     8        includes all mentions of the environment, natu...   \n",
       "     9        includes all mentions of morals, values, and r...   \n",
       "     10       includes all mentions of political leaders, va...   \n",
       "     11                   includes all mentions of youth issues   \n",
       "     12                includes all mentions of minority groups   \n",
       "     13       includes mentions of other problems or all pro...   \n",
       "     14                        includes mentions of no problems   \n",
       "     15       includes any mention that is related to don't ...   \n",
       "1940 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "\n",
       "                                                  problemissues  \\\n",
       "year problem                                                      \n",
       "1939 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "     6        Foreign policy, national security, nuclear wea...   \n",
       "     7        Globalization, trade, international cooperatio...   \n",
       "     8        Environment (general), climate change, polluti...   \n",
       "     9        Morals and values, family values, religion, cu...   \n",
       "     10       Politics, corruption, trustworthiness, leaders...   \n",
       "     11       Youths (general), crime, gangs, school violenc...   \n",
       "     12       Native Americans, hispanics, Asian Americans, ...   \n",
       "     13                                              Other, all   \n",
       "     14                                                     NaN   \n",
       "     15         Don't know, no opinion, not applicable, refusal   \n",
       "1940 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "\n",
       "                                                problemexamples subname  \\\n",
       "year problem                                                              \n",
       "1939 1        Jobs, cost of living, prices, interest rates, ...     All   \n",
       "     2        Government programs, welfare, entitlements, ai...     All   \n",
       "     3        Sexual harassment, gender equality, wage gap, ...     All   \n",
       "     4        Crime, personal security, police, rule of law,...     All   \n",
       "     5        Balance the budget, spending priorities, debt ...     All   \n",
       "     6        Foreign affairs, international problems, forei...     All   \n",
       "     7        Foreign ownership, foreign competition, trade ...     All   \n",
       "     8        Environmental protection, conservation, climat...     All   \n",
       "     9        Morals, values, ethics, honesty, family breakd...     All   \n",
       "     10       Politics as usual, abuse of power, greedy poli...     All   \n",
       "     11       Children, morals for youths, juvenile delinque...     All   \n",
       "     12       Minorities, American Indians, anti-semitism, M...     All   \n",
       "     13       Other, stress, stem cell research, computers a...     All   \n",
       "     14       Nothing in particular, no other mention, no pr...     All   \n",
       "     15       Not sure, can't say, not interested, unknown, ...     All   \n",
       "1940 1        Jobs, cost of living, prices, interest rates, ...     All   \n",
       "     2        Government programs, welfare, entitlements, ai...     All   \n",
       "     3        Sexual harassment, gender equality, wage gap, ...     All   \n",
       "     4        Crime, personal security, police, rule of law,...     All   \n",
       "     5        Balance the budget, spending priorities, debt ...     All   \n",
       "\n",
       "                      ts  \n",
       "year problem              \n",
       "1939 1        1939-01-01  \n",
       "     2        1939-01-01  \n",
       "     3        1939-01-01  \n",
       "     4        1939-01-01  \n",
       "     5        1939-01-01  \n",
       "     6        1939-01-01  \n",
       "     7        1939-01-01  \n",
       "     8        1939-01-01  \n",
       "     9        1939-01-01  \n",
       "     10       1939-01-01  \n",
       "     11       1939-01-01  \n",
       "     12       1939-01-01  \n",
       "     13       1939-01-01  \n",
       "     14       1939-01-01  \n",
       "     15       1939-01-01  \n",
       "1940 1        1940-01-01  \n",
       "     2        1940-01-01  \n",
       "     3        1940-01-01  \n",
       "     4        1940-01-01  \n",
       "     5        1940-01-01  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_data = pd.read_csv(\"C:\\\\Users\\\\alexa\\\\Code\\\\GW DATS\\\\6103 12 Intro to Data Mining\\\\Final Project\\\\data\\\\MIPD-2024-Aggregate\\\\MIPDV2-All-year.csv\",encoding=\"ANSI\",\n",
    "                         index_col=[\"year\",\"problem\"])\n",
    "issue_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b7582",
   "metadata": {},
   "source": [
    "Limit to relevant years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e46db23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>numsurveys</th>\n",
       "      <th>numrespondents</th>\n",
       "      <th>perc</th>\n",
       "      <th>catname</th>\n",
       "      <th>catdesc</th>\n",
       "      <th>problemname</th>\n",
       "      <th>problemdesc</th>\n",
       "      <th>problemissues</th>\n",
       "      <th>problemexamples</th>\n",
       "      <th>subname</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>problem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">2017</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>19.669001</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>16.531000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.969000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>15.047000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.080000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Foreign Policy</td>\n",
       "      <td>includes all mentions of foreign policy and ot...</td>\n",
       "      <td>Foreign policy, national security, nuclear wea...</td>\n",
       "      <td>Foreign affairs, international problems, forei...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>International Economic Relations</td>\n",
       "      <td>includes all mentions of international economi...</td>\n",
       "      <td>Globalization, trade, international cooperatio...</td>\n",
       "      <td>Foreign ownership, foreign competition, trade ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>2.157000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Environment</td>\n",
       "      <td>includes all mentions of the environment, natu...</td>\n",
       "      <td>Environment (general), climate change, polluti...</td>\n",
       "      <td>Environmental protection, conservation, climat...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>5.814000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Morality</td>\n",
       "      <td>includes all mentions of morals, values, and r...</td>\n",
       "      <td>Morals and values, family values, religion, cu...</td>\n",
       "      <td>Morals, values, ethics, honesty, family breakd...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>14.678000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Politics</td>\n",
       "      <td>includes all mentions of political leaders, va...</td>\n",
       "      <td>Politics, corruption, trustworthiness, leaders...</td>\n",
       "      <td>Politics as usual, abuse of power, greedy poli...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Youth Issues</td>\n",
       "      <td>includes all mentions of youth issues</td>\n",
       "      <td>Youths (general), crime, gangs, school violenc...</td>\n",
       "      <td>Children, morals for youths, juvenile delinque...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Groups</td>\n",
       "      <td>includes all mentions of minority groups</td>\n",
       "      <td>Native Americans, hispanics, Asian Americans, ...</td>\n",
       "      <td>Minorities, American Indians, anti-semitism, M...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>3.186000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Other and All</td>\n",
       "      <td>includes mentions of other problems or all pro...</td>\n",
       "      <td>Other, all</td>\n",
       "      <td>Other, stress, stem cell research, computers a...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>includes mentions of no problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing in particular, no other mention, no pr...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.676000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Don't Know/Refused</td>\n",
       "      <td>includes any mention that is related to don't ...</td>\n",
       "      <td>Don't know, no opinion, not applicable, refusal</td>\n",
       "      <td>Not sure, can't say, not interested, unknown, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">2018</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>14.319000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>11.224000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>8.407000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>3.457000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>15.009000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>12.685000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Foreign Policy</td>\n",
       "      <td>includes all mentions of foreign policy and ot...</td>\n",
       "      <td>Foreign policy, national security, nuclear wea...</td>\n",
       "      <td>Foreign affairs, international problems, forei...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>International Economic Relations</td>\n",
       "      <td>includes all mentions of international economi...</td>\n",
       "      <td>Globalization, trade, international cooperatio...</td>\n",
       "      <td>Foreign ownership, foreign competition, trade ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.959000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Environment</td>\n",
       "      <td>includes all mentions of the environment, natu...</td>\n",
       "      <td>Environment (general), climate change, polluti...</td>\n",
       "      <td>Environmental protection, conservation, climat...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Morality</td>\n",
       "      <td>includes all mentions of morals, values, and r...</td>\n",
       "      <td>Morals and values, family values, religion, cu...</td>\n",
       "      <td>Morals, values, ethics, honesty, family breakd...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>17.754000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Politics</td>\n",
       "      <td>includes all mentions of political leaders, va...</td>\n",
       "      <td>Politics, corruption, trustworthiness, leaders...</td>\n",
       "      <td>Politics as usual, abuse of power, greedy poli...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Youth Issues</td>\n",
       "      <td>includes all mentions of youth issues</td>\n",
       "      <td>Youths (general), crime, gangs, school violenc...</td>\n",
       "      <td>Children, morals for youths, juvenile delinque...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Groups</td>\n",
       "      <td>includes all mentions of minority groups</td>\n",
       "      <td>Native Americans, hispanics, Asian Americans, ...</td>\n",
       "      <td>Minorities, American Indians, anti-semitism, M...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>4.345000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Other and All</td>\n",
       "      <td>includes mentions of other problems or all pro...</td>\n",
       "      <td>Other, all</td>\n",
       "      <td>Other, stress, stem cell research, computers a...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>includes mentions of no problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing in particular, no other mention, no pr...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>4.728000</td>\n",
       "      <td>MIPD (General)</td>\n",
       "      <td>general problem categories in the MIPD</td>\n",
       "      <td>Don't Know/Refused</td>\n",
       "      <td>includes any mention that is related to don't ...</td>\n",
       "      <td>Don't know, no opinion, not applicable, refusal</td>\n",
       "      <td>Not sure, can't say, not interested, unknown, ...</td>\n",
       "      <td>All</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cat  numsurveys  numrespondents       perc         catname  \\\n",
       "year problem                                                               \n",
       "2017 1          1           2            1999  19.669001  MIPD (General)   \n",
       "     2          1           2            1999  16.531000  MIPD (General)   \n",
       "     3          1           2            1999   4.969000  MIPD (General)   \n",
       "     4          1           2            1999   1.275000  MIPD (General)   \n",
       "     5          1           2            1999  15.047000  MIPD (General)   \n",
       "     6          1           2            1999  11.080000  MIPD (General)   \n",
       "     7          1           2            1999   0.222000  MIPD (General)   \n",
       "     8          1           2            1999   2.157000  MIPD (General)   \n",
       "     9          1           2            1999   5.814000  MIPD (General)   \n",
       "     10         1           2            1999  14.678000  MIPD (General)   \n",
       "     11         1           2            1999   0.426000  MIPD (General)   \n",
       "     12         1           2            1999   0.000000  MIPD (General)   \n",
       "     13         1           2            1999   3.186000  MIPD (General)   \n",
       "     14         1           2            1999   0.269000  MIPD (General)   \n",
       "     15         1           2            1999   4.676000  MIPD (General)   \n",
       "2018 1          1           2            1983  14.319000  MIPD (General)   \n",
       "     2          1           2            1983  11.224000  MIPD (General)   \n",
       "     3          1           2            1983   8.407000  MIPD (General)   \n",
       "     4          1           2            1983   3.457000  MIPD (General)   \n",
       "     5          1           2            1983  15.009000  MIPD (General)   \n",
       "     6          1           2            1983  12.685000  MIPD (General)   \n",
       "     7          1           2            1983   0.282000  MIPD (General)   \n",
       "     8          1           2            1983   1.959000  MIPD (General)   \n",
       "     9          1           2            1983   5.000000  MIPD (General)   \n",
       "     10         1           2            1983  17.754000  MIPD (General)   \n",
       "     11         1           2            1983   0.707000  MIPD (General)   \n",
       "     12         1           2            1983   0.000000  MIPD (General)   \n",
       "     13         1           2            1983   4.345000  MIPD (General)   \n",
       "     14         1           2            1983   0.125000  MIPD (General)   \n",
       "     15         1           2            1983   4.728000  MIPD (General)   \n",
       "\n",
       "                                             catdesc  \\\n",
       "year problem                                           \n",
       "2017 1        general problem categories in the MIPD   \n",
       "     2        general problem categories in the MIPD   \n",
       "     3        general problem categories in the MIPD   \n",
       "     4        general problem categories in the MIPD   \n",
       "     5        general problem categories in the MIPD   \n",
       "     6        general problem categories in the MIPD   \n",
       "     7        general problem categories in the MIPD   \n",
       "     8        general problem categories in the MIPD   \n",
       "     9        general problem categories in the MIPD   \n",
       "     10       general problem categories in the MIPD   \n",
       "     11       general problem categories in the MIPD   \n",
       "     12       general problem categories in the MIPD   \n",
       "     13       general problem categories in the MIPD   \n",
       "     14       general problem categories in the MIPD   \n",
       "     15       general problem categories in the MIPD   \n",
       "2018 1        general problem categories in the MIPD   \n",
       "     2        general problem categories in the MIPD   \n",
       "     3        general problem categories in the MIPD   \n",
       "     4        general problem categories in the MIPD   \n",
       "     5        general problem categories in the MIPD   \n",
       "     6        general problem categories in the MIPD   \n",
       "     7        general problem categories in the MIPD   \n",
       "     8        general problem categories in the MIPD   \n",
       "     9        general problem categories in the MIPD   \n",
       "     10       general problem categories in the MIPD   \n",
       "     11       general problem categories in the MIPD   \n",
       "     12       general problem categories in the MIPD   \n",
       "     13       general problem categories in the MIPD   \n",
       "     14       general problem categories in the MIPD   \n",
       "     15       general problem categories in the MIPD   \n",
       "\n",
       "                                   problemname  \\\n",
       "year problem                                     \n",
       "2017 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "     6                          Foreign Policy   \n",
       "     7        International Economic Relations   \n",
       "     8                             Environment   \n",
       "     9                                Morality   \n",
       "     10                               Politics   \n",
       "     11                           Youth Issues   \n",
       "     12                                 Groups   \n",
       "     13                          Other and All   \n",
       "     14                                    NaN   \n",
       "     15                     Don't Know/Refused   \n",
       "2018 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "     6                          Foreign Policy   \n",
       "     7        International Economic Relations   \n",
       "     8                             Environment   \n",
       "     9                                Morality   \n",
       "     10                               Politics   \n",
       "     11                           Youth Issues   \n",
       "     12                                 Groups   \n",
       "     13                          Other and All   \n",
       "     14                                    NaN   \n",
       "     15                     Don't Know/Refused   \n",
       "\n",
       "                                                    problemdesc  \\\n",
       "year problem                                                      \n",
       "2017 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "     6        includes all mentions of foreign policy and ot...   \n",
       "     7        includes all mentions of international economi...   \n",
       "     8        includes all mentions of the environment, natu...   \n",
       "     9        includes all mentions of morals, values, and r...   \n",
       "     10       includes all mentions of political leaders, va...   \n",
       "     11                   includes all mentions of youth issues   \n",
       "     12                includes all mentions of minority groups   \n",
       "     13       includes mentions of other problems or all pro...   \n",
       "     14                        includes mentions of no problems   \n",
       "     15       includes any mention that is related to don't ...   \n",
       "2018 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "     6        includes all mentions of foreign policy and ot...   \n",
       "     7        includes all mentions of international economi...   \n",
       "     8        includes all mentions of the environment, natu...   \n",
       "     9        includes all mentions of morals, values, and r...   \n",
       "     10       includes all mentions of political leaders, va...   \n",
       "     11                   includes all mentions of youth issues   \n",
       "     12                includes all mentions of minority groups   \n",
       "     13       includes mentions of other problems or all pro...   \n",
       "     14                        includes mentions of no problems   \n",
       "     15       includes any mention that is related to don't ...   \n",
       "\n",
       "                                                  problemissues  \\\n",
       "year problem                                                      \n",
       "2017 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "     6        Foreign policy, national security, nuclear wea...   \n",
       "     7        Globalization, trade, international cooperatio...   \n",
       "     8        Environment (general), climate change, polluti...   \n",
       "     9        Morals and values, family values, religion, cu...   \n",
       "     10       Politics, corruption, trustworthiness, leaders...   \n",
       "     11       Youths (general), crime, gangs, school violenc...   \n",
       "     12       Native Americans, hispanics, Asian Americans, ...   \n",
       "     13                                              Other, all   \n",
       "     14                                                     NaN   \n",
       "     15         Don't know, no opinion, not applicable, refusal   \n",
       "2018 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "     6        Foreign policy, national security, nuclear wea...   \n",
       "     7        Globalization, trade, international cooperatio...   \n",
       "     8        Environment (general), climate change, polluti...   \n",
       "     9        Morals and values, family values, religion, cu...   \n",
       "     10       Politics, corruption, trustworthiness, leaders...   \n",
       "     11       Youths (general), crime, gangs, school violenc...   \n",
       "     12       Native Americans, hispanics, Asian Americans, ...   \n",
       "     13                                              Other, all   \n",
       "     14                                                     NaN   \n",
       "     15         Don't know, no opinion, not applicable, refusal   \n",
       "\n",
       "                                                problemexamples subname  \\\n",
       "year problem                                                              \n",
       "2017 1        Jobs, cost of living, prices, interest rates, ...     All   \n",
       "     2        Government programs, welfare, entitlements, ai...     All   \n",
       "     3        Sexual harassment, gender equality, wage gap, ...     All   \n",
       "     4        Crime, personal security, police, rule of law,...     All   \n",
       "     5        Balance the budget, spending priorities, debt ...     All   \n",
       "     6        Foreign affairs, international problems, forei...     All   \n",
       "     7        Foreign ownership, foreign competition, trade ...     All   \n",
       "     8        Environmental protection, conservation, climat...     All   \n",
       "     9        Morals, values, ethics, honesty, family breakd...     All   \n",
       "     10       Politics as usual, abuse of power, greedy poli...     All   \n",
       "     11       Children, morals for youths, juvenile delinque...     All   \n",
       "     12       Minorities, American Indians, anti-semitism, M...     All   \n",
       "     13       Other, stress, stem cell research, computers a...     All   \n",
       "     14       Nothing in particular, no other mention, no pr...     All   \n",
       "     15       Not sure, can't say, not interested, unknown, ...     All   \n",
       "2018 1        Jobs, cost of living, prices, interest rates, ...     All   \n",
       "     2        Government programs, welfare, entitlements, ai...     All   \n",
       "     3        Sexual harassment, gender equality, wage gap, ...     All   \n",
       "     4        Crime, personal security, police, rule of law,...     All   \n",
       "     5        Balance the budget, spending priorities, debt ...     All   \n",
       "     6        Foreign affairs, international problems, forei...     All   \n",
       "     7        Foreign ownership, foreign competition, trade ...     All   \n",
       "     8        Environmental protection, conservation, climat...     All   \n",
       "     9        Morals, values, ethics, honesty, family breakd...     All   \n",
       "     10       Politics as usual, abuse of power, greedy poli...     All   \n",
       "     11       Children, morals for youths, juvenile delinque...     All   \n",
       "     12       Minorities, American Indians, anti-semitism, M...     All   \n",
       "     13       Other, stress, stem cell research, computers a...     All   \n",
       "     14       Nothing in particular, no other mention, no pr...     All   \n",
       "     15       Not sure, can't say, not interested, unknown, ...     All   \n",
       "\n",
       "                      ts  \n",
       "year problem              \n",
       "2017 1        2017-01-01  \n",
       "     2        2017-01-01  \n",
       "     3        2017-01-01  \n",
       "     4        2017-01-01  \n",
       "     5        2017-01-01  \n",
       "     6        2017-01-01  \n",
       "     7        2017-01-01  \n",
       "     8        2017-01-01  \n",
       "     9        2017-01-01  \n",
       "     10       2017-01-01  \n",
       "     11       2017-01-01  \n",
       "     12       2017-01-01  \n",
       "     13       2017-01-01  \n",
       "     14       2017-01-01  \n",
       "     15       2017-01-01  \n",
       "2018 1        2018-01-01  \n",
       "     2        2018-01-01  \n",
       "     3        2018-01-01  \n",
       "     4        2018-01-01  \n",
       "     5        2018-01-01  \n",
       "     6        2018-01-01  \n",
       "     7        2018-01-01  \n",
       "     8        2018-01-01  \n",
       "     9        2018-01-01  \n",
       "     10       2018-01-01  \n",
       "     11       2018-01-01  \n",
       "     12       2018-01-01  \n",
       "     13       2018-01-01  \n",
       "     14       2018-01-01  \n",
       "     15       2018-01-01  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_data = issue_data.loc[[2017,2018]]\n",
    "issue_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04571cf",
   "metadata": {},
   "source": [
    "### Merge the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c9df1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>Metropolitan Division Code</th>\n",
       "      <th>CSA Code</th>\n",
       "      <th>CBSA Title</th>\n",
       "      <th>Metropolitan/Micropolitan Statistical Area</th>\n",
       "      <th>Metropolitan Division Title</th>\n",
       "      <th>CSA Title</th>\n",
       "      <th>County/County Equivalent</th>\n",
       "      <th>State Name</th>\n",
       "      <th>FIPS State Code</th>\n",
       "      <th>FIPS County Code</th>\n",
       "      <th>Central/Outlying County</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>Micropolitan Statistical Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brown County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>013</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aberdeen, SD</td>\n",
       "      <td>Micropolitan Statistical Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edmunds County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>045</td>\n",
       "      <td>Outlying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aberdeen, WA</td>\n",
       "      <td>Micropolitan Statistical Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grays Harbor County</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53</td>\n",
       "      <td>027</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>Abilene, TX</td>\n",
       "      <td>Metropolitan Statistical Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abilene-Sweetwater, TX</td>\n",
       "      <td>Callahan County</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48</td>\n",
       "      <td>059</td>\n",
       "      <td>Outlying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101</td>\n",
       "      <td>Abilene, TX</td>\n",
       "      <td>Metropolitan Statistical Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abilene-Sweetwater, TX</td>\n",
       "      <td>Jones County</td>\n",
       "      <td>Texas</td>\n",
       "      <td>48</td>\n",
       "      <td>253</td>\n",
       "      <td>Outlying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CBSA Code Metropolitan Division Code CSA Code    CBSA Title  \\\n",
       "0     10100                        NaN      NaN  Aberdeen, SD   \n",
       "1     10100                        NaN      NaN  Aberdeen, SD   \n",
       "2     10140                        NaN      NaN  Aberdeen, WA   \n",
       "3     10180                        NaN      101   Abilene, TX   \n",
       "4     10180                        NaN      101   Abilene, TX   \n",
       "\n",
       "  Metropolitan/Micropolitan Statistical Area Metropolitan Division Title  \\\n",
       "0              Micropolitan Statistical Area                         NaN   \n",
       "1              Micropolitan Statistical Area                         NaN   \n",
       "2              Micropolitan Statistical Area                         NaN   \n",
       "3              Metropolitan Statistical Area                         NaN   \n",
       "4              Metropolitan Statistical Area                         NaN   \n",
       "\n",
       "                CSA Title County/County Equivalent    State Name  \\\n",
       "0                     NaN             Brown County  South Dakota   \n",
       "1                     NaN           Edmunds County  South Dakota   \n",
       "2                     NaN      Grays Harbor County    Washington   \n",
       "3  Abilene-Sweetwater, TX          Callahan County         Texas   \n",
       "4  Abilene-Sweetwater, TX             Jones County         Texas   \n",
       "\n",
       "  FIPS State Code FIPS County Code Central/Outlying County  \n",
       "0              46              013                 Central  \n",
       "1              46              045                Outlying  \n",
       "2              53              027                 Central  \n",
       "3              48              059                Outlying  \n",
       "4              48              253                Outlying  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_fips_crosswalk = pd.read_excel(\"C:\\\\Users\\\\alexa\\\\Code\\\\GW DATS\\\\6103 12 Intro to Data Mining\\\\Final Project\\\\data\\\\list1_2023.xlsx\",skiprows=2,dtype=str)\n",
    "msa_fips_crosswalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c7e79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_fips_crosswalk[\"FIPS Code\"] = msa_fips_crosswalk[\"FIPS State Code\"] + msa_fips_crosswalk[\"FIPS County Code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25902b",
   "metadata": {},
   "source": [
    "Drop NaN FIPS codes in preparation for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f08163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       46013\n",
       "1       46045\n",
       "2       53027\n",
       "3       48059\n",
       "4       48253\n",
       "        ...  \n",
       "1910     6101\n",
       "1911     6115\n",
       "1912     4027\n",
       "1913    39119\n",
       "1914    48505\n",
       "Name: FIPS Code, Length: 1915, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_fips_crosswalk = msa_fips_crosswalk[msa_fips_crosswalk[\"FIPS Code\"].notna()]\n",
    "msa_fips_crosswalk[\"FIPS Code\"] = msa_fips_crosswalk[\"FIPS Code\"].astype(int)\n",
    "msa_fips_crosswalk[\"FIPS Code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cec6f77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>banking and finance; economy; energy; environm...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>policing</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>33100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>education; racism</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>11460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>38085.0</td>\n",
       "      <td>protest; occupying land</td>\n",
       "      <td>energy; environment</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>presidency</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>35620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  fips_code                     type  \\\n",
       "0  2017    27053.0     protest; banner drop   \n",
       "1  2017    12086.0            demonstration   \n",
       "2  2017    26161.0                  protest   \n",
       "3  2017    38085.0  protest; occupying land   \n",
       "4  2017    36061.0                  protest   \n",
       "\n",
       "                                              issues  size_mean  size_cat  \\\n",
       "0  banking and finance; economy; energy; environm...        2.0         1   \n",
       "1                                           policing       18.0         1   \n",
       "2                                  education; racism      200.0         2   \n",
       "3                                energy; environment      300.0         2   \n",
       "4                                         presidency        2.0         1   \n",
       "\n",
       "   FIPS Code CBSA Code  \n",
       "0    27053.0     33460  \n",
       "1    12086.0     33100  \n",
       "2    26161.0     11460  \n",
       "3        NaN       NaN  \n",
       "4    36061.0     35620  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_data_msa = crowd_data.merge(msa_fips_crosswalk[[\"FIPS Code\", \"CBSA Code\"]], how=\"left\", left_on=\"fips_code\", right_on=\"FIPS Code\")\n",
    "crowd_data_msa.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dd097e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_census_data = crowd_data_msa.merge(census_data, how=\"inner\", \n",
    "                                         left_on = [\"year\",\"CBSA Code\"], right_on = [\"year\",\"METROPOLITAN_STATISTICAL_AREA_MICROPOLITAN_STATISTICAL_AREA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1302d252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DP02PR_0001PE</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0080PE</th>\n",
       "      <th>DP05_0081PE</th>\n",
       "      <th>DP05_0082PE</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>banking and finance; economy; energy; environm...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>policing</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2534993</td>\n",
       "      <td>3924471</td>\n",
       "      <td>47.4</td>\n",
       "      <td>52.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>education; racism</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>11460</td>\n",
       "      <td>Ann Arbor, MI Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150397</td>\n",
       "      <td>278978</td>\n",
       "      <td>49.2</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>presidency</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>35620</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7983770</td>\n",
       "      <td>13563604</td>\n",
       "      <td>47.2</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>presidency</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>47900</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2354827</td>\n",
       "      <td>4155063</td>\n",
       "      <td>48.2</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  fips_code                  type  \\\n",
       "0  2017    27053.0  protest; banner drop   \n",
       "1  2017    12086.0         demonstration   \n",
       "2  2017    26161.0               protest   \n",
       "3  2017    36061.0               protest   \n",
       "4  2017    11001.0               protest   \n",
       "\n",
       "                                              issues  size_mean  size_cat  \\\n",
       "0  banking and finance; economy; energy; environm...        2.0         1   \n",
       "1                                           policing       18.0         1   \n",
       "2                                  education; racism      200.0         2   \n",
       "3                                         presidency        2.0         1   \n",
       "4                                         presidency        3.0         1   \n",
       "\n",
       "   FIPS Code CBSA Code                                               NAME  \\\n",
       "0    27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "1    12086.0     33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "2    26161.0     11460                           Ann Arbor, MI Metro Area   \n",
       "3    36061.0     35620   New York-Newark-Jersey City, NY-NJ-PA Metro Area   \n",
       "4    11001.0     47900  Washington-Arlington-Alexandria, DC-VA-MD-WV M...   \n",
       "\n",
       "   DP02PR_0001PE  ...  DP05_0080PE  DP05_0081PE  DP05_0082PE  DP05_0083PE  \\\n",
       "0            NaN  ...          6.7          0.0          0.1          2.7   \n",
       "1            NaN  ...          2.4          0.0          0.4          1.3   \n",
       "2            NaN  ...          9.0          0.0          0.3          4.1   \n",
       "3            NaN  ...         11.3          0.0          0.7          1.7   \n",
       "4            NaN  ...         10.2          0.0          0.4          3.3   \n",
       "\n",
       "   DP05_0084PE  DP05_0085PE  DP05_0086E  DP05_0087PE  DP05_0088PE  DP05_0089PE  \n",
       "0          0.1          2.6     1447758      2591001         49.0         51.0  \n",
       "1          0.2          1.1     2534993      3924471         47.4         52.6  \n",
       "2          0.1          4.0      150397       278978         49.2         50.8  \n",
       "3          0.2          1.5     7983770     13563604         47.2         52.8  \n",
       "4          0.2          3.1     2354827      4155063         48.2         51.8  \n",
       "\n",
       "[5 rows x 678 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7036fd",
   "metadata": {},
   "source": [
    "#### Merge on Issue Salience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a63e2",
   "metadata": {},
   "source": [
    "Some crowds are gathered for issues that fit into multiple categories, organized in alphabetical order. If this is the case, how should they be merged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9786328",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_census_data[\"issues\"] = crowd_census_data[\"issues\"].apply(lambda x:np.array(x.split(\"; \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ea75018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [banking and finance, economy, energy, environ...\n",
       "1                                               [policing]\n",
       "2                                      [education, racism]\n",
       "3                                             [presidency]\n",
       "4                                             [presidency]\n",
       "                               ...                        \n",
       "14880                                         [presidency]\n",
       "14881                                  [education, racism]\n",
       "14882                                    [foreign affairs]\n",
       "14883                                   [policing, racism]\n",
       "14884                                            [housing]\n",
       "Name: issues, Length: 14885, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data[\"issues\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112dca8",
   "metadata": {},
   "source": [
    "Perhaps solely the issue with the highest salience should be accounted for? Or, perhaps, the pcts should be added, representing the salience of anyone interested in at least one of the issues?\n",
    "\n",
    "\"The percentages can therefore be interpreted as the percentage of the electorate who answered the question and selected that category.\" -MIPDV2-Codebook p. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f913bd4",
   "metadata": {},
   "source": [
    "The latter will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cc5f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animal rights', 'banking and finance', 'civil rights',\n",
       "       'corruption', 'covid', 'criminal justice', 'democracy',\n",
       "       'development', 'disability rights', 'drugs', 'economy',\n",
       "       'education', 'energy', 'environment', 'foreign affairs',\n",
       "       'free speech', 'guns', 'healthcare', 'housing', 'immigration',\n",
       "       'indigenous peoples', 'judiciary', 'labor', 'legislative',\n",
       "       'lgbtqia', 'military', 'patriotism', 'policing', 'presidency',\n",
       "       'racism', 'religion', 'reproductive rights', 'science',\n",
       "       'sexual violence', 'sports', 'taxes', 'transportation',\n",
       "       \"women's rights\"], dtype='<U19')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.concat(crowd_census_data[\"issues\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267c669",
   "metadata": {},
   "source": [
    "All the issues in the CCC data. Each is to be mapped to a problem code in the MIPD data. This will be done by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773692d9",
   "metadata": {},
   "source": [
    "But why is `covid` there? This should be limited to the years 2017 and 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1cdd473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DP02PR_0001PE</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0080PE</th>\n",
       "      <th>DP05_0081PE</th>\n",
       "      <th>DP05_0082PE</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>2017</td>\n",
       "      <td>8031.0</td>\n",
       "      <td>rally</td>\n",
       "      <td>[covid, judiciary]</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8031.0</td>\n",
       "      <td>19740</td>\n",
       "      <td>Denver-Aurora-Lakewood, CO Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1173413</td>\n",
       "      <td>2043836</td>\n",
       "      <td>49.2</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11871</th>\n",
       "      <td>2018</td>\n",
       "      <td>13121.0</td>\n",
       "      <td>protest; rally</td>\n",
       "      <td>[covid, healthcare]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>13121.0</td>\n",
       "      <td>12060</td>\n",
       "      <td>Atlanta-Sandy Springs-Roswell, GA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2331002</td>\n",
       "      <td>4111190</td>\n",
       "      <td>47.1</td>\n",
       "      <td>52.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13342</th>\n",
       "      <td>2018</td>\n",
       "      <td>6075.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[covid, energy]</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6075.0</td>\n",
       "      <td>41860</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1825205</td>\n",
       "      <td>3230062</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 678 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  fips_code            type               issues  size_mean  \\\n",
       "4099   2017     8031.0           rally   [covid, judiciary]       20.0   \n",
       "11871  2018    13121.0  protest; rally  [covid, healthcare]      100.0   \n",
       "13342  2018     6075.0         protest      [covid, energy]       11.0   \n",
       "\n",
       "       size_cat  FIPS Code CBSA Code  \\\n",
       "4099          1     8031.0     19740   \n",
       "11871         2    13121.0     12060   \n",
       "13342         1     6075.0     41860   \n",
       "\n",
       "                                               NAME  DP02PR_0001PE  ...  \\\n",
       "4099          Denver-Aurora-Lakewood, CO Metro Area            NaN  ...   \n",
       "11871  Atlanta-Sandy Springs-Roswell, GA Metro Area            NaN  ...   \n",
       "13342  San Francisco-Oakland-Hayward, CA Metro Area            NaN  ...   \n",
       "\n",
       "       DP05_0080PE  DP05_0081PE  DP05_0082PE  DP05_0083PE  DP05_0084PE  \\\n",
       "4099           4.3          0.1          0.2          2.2          0.1   \n",
       "11871          6.1          0.1          0.4          2.2          0.1   \n",
       "13342         26.3          0.6          0.4          4.5          0.2   \n",
       "\n",
       "       DP05_0085PE  DP05_0086E  DP05_0087PE  DP05_0088PE  DP05_0089PE  \n",
       "4099           2.2     1173413      2043836         49.2         50.8  \n",
       "11871          2.1     2331002      4111190         47.1         52.9  \n",
       "13342          4.3     1825205      3230062         49.0         51.0  \n",
       "\n",
       "[3 rows x 678 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data_covid = crowd_census_data[\"issues\"].apply(lambda x:\"covid\" in x)\n",
    "crowd_census_data[crowd_census_data_covid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc85fb3",
   "metadata": {},
   "source": [
    "According to the data dictionary, \"These are generated after data compilation by running a series of regular expressions over the claims description text\" (p. 3). These claims all include the words \"reopen\" or \"reopening\"; these are false positives. As such, `covid` will manually be removed from these four so as to not impact findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b78c987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4099      [judiciary]\n",
       "11871    [healthcare]\n",
       "13342        [energy]\n",
       "Name: issues, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data.loc[crowd_census_data_covid,\"issues\"] = crowd_census_data.loc[crowd_census_data_covid, \"issues\"].apply(lambda x:np.setdiff1d(x, (\"covid\")))\n",
    "crowd_census_data.loc[crowd_census_data_covid,\"issues\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffba2e",
   "metadata": {},
   "source": [
    "Check again for unique issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39a068fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animal rights', 'banking and finance', 'civil rights',\n",
       "       'corruption', 'criminal justice', 'democracy', 'development',\n",
       "       'disability rights', 'drugs', 'economy', 'education', 'energy',\n",
       "       'environment', 'foreign affairs', 'free speech', 'guns',\n",
       "       'healthcare', 'housing', 'immigration', 'indigenous peoples',\n",
       "       'judiciary', 'labor', 'legislative', 'lgbtqia', 'military',\n",
       "       'patriotism', 'policing', 'presidency', 'racism', 'religion',\n",
       "       'reproductive rights', 'science', 'sexual violence', 'sports',\n",
       "       'taxes', 'transportation', \"women's rights\"], dtype='<U19')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.concat(crowd_census_data[\"issues\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25bc8115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>problemname</th>\n",
       "      <th>problemdesc</th>\n",
       "      <th>problemissues</th>\n",
       "      <th>problemexamples</th>\n",
       "      <th>perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>problem</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">2017</th>\n",
       "      <th>1</th>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>19.669001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>16.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>4.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>1.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>15.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Foreign Policy</td>\n",
       "      <td>includes all mentions of foreign policy and ot...</td>\n",
       "      <td>Foreign policy, national security, nuclear wea...</td>\n",
       "      <td>Foreign affairs, international problems, forei...</td>\n",
       "      <td>11.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>International Economic Relations</td>\n",
       "      <td>includes all mentions of international economi...</td>\n",
       "      <td>Globalization, trade, international cooperatio...</td>\n",
       "      <td>Foreign ownership, foreign competition, trade ...</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Environment</td>\n",
       "      <td>includes all mentions of the environment, natu...</td>\n",
       "      <td>Environment (general), climate change, polluti...</td>\n",
       "      <td>Environmental protection, conservation, climat...</td>\n",
       "      <td>2.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Morality</td>\n",
       "      <td>includes all mentions of morals, values, and r...</td>\n",
       "      <td>Morals and values, family values, religion, cu...</td>\n",
       "      <td>Morals, values, ethics, honesty, family breakd...</td>\n",
       "      <td>5.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Politics</td>\n",
       "      <td>includes all mentions of political leaders, va...</td>\n",
       "      <td>Politics, corruption, trustworthiness, leaders...</td>\n",
       "      <td>Politics as usual, abuse of power, greedy poli...</td>\n",
       "      <td>14.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Youth Issues</td>\n",
       "      <td>includes all mentions of youth issues</td>\n",
       "      <td>Youths (general), crime, gangs, school violenc...</td>\n",
       "      <td>Children, morals for youths, juvenile delinque...</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Groups</td>\n",
       "      <td>includes all mentions of minority groups</td>\n",
       "      <td>Native Americans, hispanics, Asian Americans, ...</td>\n",
       "      <td>Minorities, American Indians, anti-semitism, M...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Other and All</td>\n",
       "      <td>includes mentions of other problems or all pro...</td>\n",
       "      <td>Other, all</td>\n",
       "      <td>Other, stress, stem cell research, computers a...</td>\n",
       "      <td>3.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>includes mentions of no problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing in particular, no other mention, no pr...</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Don't Know/Refused</td>\n",
       "      <td>includes any mention that is related to don't ...</td>\n",
       "      <td>Don't know, no opinion, not applicable, refusal</td>\n",
       "      <td>Not sure, can't say, not interested, unknown, ...</td>\n",
       "      <td>4.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"15\" valign=\"top\">2018</th>\n",
       "      <th>1</th>\n",
       "      <td>Economy</td>\n",
       "      <td>includes all mentions of economic issues</td>\n",
       "      <td>Economy (general), unemployment, inflation, gr...</td>\n",
       "      <td>Jobs, cost of living, prices, interest rates, ...</td>\n",
       "      <td>14.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Social Policy</td>\n",
       "      <td>includes all mentions of social policy</td>\n",
       "      <td>Social policy (general), welfare, education, h...</td>\n",
       "      <td>Government programs, welfare, entitlements, ai...</td>\n",
       "      <td>11.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rights</td>\n",
       "      <td>includes all mentions of civil and political r...</td>\n",
       "      <td>Gender rights, LGBTQ+ rights, reproductive rig...</td>\n",
       "      <td>Sexual harassment, gender equality, wage gap, ...</td>\n",
       "      <td>8.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Public Safety</td>\n",
       "      <td>includes all mentions of crime and public safety</td>\n",
       "      <td>Crime, law enforcement, gun control, drugs, al...</td>\n",
       "      <td>Crime, personal security, police, rule of law,...</td>\n",
       "      <td>3.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fiscal Policy</td>\n",
       "      <td>includes all mentions of budgets and spending</td>\n",
       "      <td>Budget, debt, deficit, government spending, so...</td>\n",
       "      <td>Balance the budget, spending priorities, debt ...</td>\n",
       "      <td>15.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Foreign Policy</td>\n",
       "      <td>includes all mentions of foreign policy and ot...</td>\n",
       "      <td>Foreign policy, national security, nuclear wea...</td>\n",
       "      <td>Foreign affairs, international problems, forei...</td>\n",
       "      <td>12.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>International Economic Relations</td>\n",
       "      <td>includes all mentions of international economi...</td>\n",
       "      <td>Globalization, trade, international cooperatio...</td>\n",
       "      <td>Foreign ownership, foreign competition, trade ...</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Environment</td>\n",
       "      <td>includes all mentions of the environment, natu...</td>\n",
       "      <td>Environment (general), climate change, polluti...</td>\n",
       "      <td>Environmental protection, conservation, climat...</td>\n",
       "      <td>1.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Morality</td>\n",
       "      <td>includes all mentions of morals, values, and r...</td>\n",
       "      <td>Morals and values, family values, religion, cu...</td>\n",
       "      <td>Morals, values, ethics, honesty, family breakd...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Politics</td>\n",
       "      <td>includes all mentions of political leaders, va...</td>\n",
       "      <td>Politics, corruption, trustworthiness, leaders...</td>\n",
       "      <td>Politics as usual, abuse of power, greedy poli...</td>\n",
       "      <td>17.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Youth Issues</td>\n",
       "      <td>includes all mentions of youth issues</td>\n",
       "      <td>Youths (general), crime, gangs, school violenc...</td>\n",
       "      <td>Children, morals for youths, juvenile delinque...</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Groups</td>\n",
       "      <td>includes all mentions of minority groups</td>\n",
       "      <td>Native Americans, hispanics, Asian Americans, ...</td>\n",
       "      <td>Minorities, American Indians, anti-semitism, M...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Other and All</td>\n",
       "      <td>includes mentions of other problems or all pro...</td>\n",
       "      <td>Other, all</td>\n",
       "      <td>Other, stress, stem cell research, computers a...</td>\n",
       "      <td>4.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>includes mentions of no problems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing in particular, no other mention, no pr...</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Don't Know/Refused</td>\n",
       "      <td>includes any mention that is related to don't ...</td>\n",
       "      <td>Don't know, no opinion, not applicable, refusal</td>\n",
       "      <td>Not sure, can't say, not interested, unknown, ...</td>\n",
       "      <td>4.728000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   problemname  \\\n",
       "year problem                                     \n",
       "2017 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "     6                          Foreign Policy   \n",
       "     7        International Economic Relations   \n",
       "     8                             Environment   \n",
       "     9                                Morality   \n",
       "     10                               Politics   \n",
       "     11                           Youth Issues   \n",
       "     12                                 Groups   \n",
       "     13                          Other and All   \n",
       "     14                                    NaN   \n",
       "     15                     Don't Know/Refused   \n",
       "2018 1                                 Economy   \n",
       "     2                           Social Policy   \n",
       "     3                                  Rights   \n",
       "     4                           Public Safety   \n",
       "     5                           Fiscal Policy   \n",
       "     6                          Foreign Policy   \n",
       "     7        International Economic Relations   \n",
       "     8                             Environment   \n",
       "     9                                Morality   \n",
       "     10                               Politics   \n",
       "     11                           Youth Issues   \n",
       "     12                                 Groups   \n",
       "     13                          Other and All   \n",
       "     14                                    NaN   \n",
       "     15                     Don't Know/Refused   \n",
       "\n",
       "                                                    problemdesc  \\\n",
       "year problem                                                      \n",
       "2017 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "     6        includes all mentions of foreign policy and ot...   \n",
       "     7        includes all mentions of international economi...   \n",
       "     8        includes all mentions of the environment, natu...   \n",
       "     9        includes all mentions of morals, values, and r...   \n",
       "     10       includes all mentions of political leaders, va...   \n",
       "     11                   includes all mentions of youth issues   \n",
       "     12                includes all mentions of minority groups   \n",
       "     13       includes mentions of other problems or all pro...   \n",
       "     14                        includes mentions of no problems   \n",
       "     15       includes any mention that is related to don't ...   \n",
       "2018 1                 includes all mentions of economic issues   \n",
       "     2                   includes all mentions of social policy   \n",
       "     3        includes all mentions of civil and political r...   \n",
       "     4         includes all mentions of crime and public safety   \n",
       "     5            includes all mentions of budgets and spending   \n",
       "     6        includes all mentions of foreign policy and ot...   \n",
       "     7        includes all mentions of international economi...   \n",
       "     8        includes all mentions of the environment, natu...   \n",
       "     9        includes all mentions of morals, values, and r...   \n",
       "     10       includes all mentions of political leaders, va...   \n",
       "     11                   includes all mentions of youth issues   \n",
       "     12                includes all mentions of minority groups   \n",
       "     13       includes mentions of other problems or all pro...   \n",
       "     14                        includes mentions of no problems   \n",
       "     15       includes any mention that is related to don't ...   \n",
       "\n",
       "                                                  problemissues  \\\n",
       "year problem                                                      \n",
       "2017 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "     6        Foreign policy, national security, nuclear wea...   \n",
       "     7        Globalization, trade, international cooperatio...   \n",
       "     8        Environment (general), climate change, polluti...   \n",
       "     9        Morals and values, family values, religion, cu...   \n",
       "     10       Politics, corruption, trustworthiness, leaders...   \n",
       "     11       Youths (general), crime, gangs, school violenc...   \n",
       "     12       Native Americans, hispanics, Asian Americans, ...   \n",
       "     13                                              Other, all   \n",
       "     14                                                     NaN   \n",
       "     15         Don't know, no opinion, not applicable, refusal   \n",
       "2018 1        Economy (general), unemployment, inflation, gr...   \n",
       "     2        Social policy (general), welfare, education, h...   \n",
       "     3        Gender rights, LGBTQ+ rights, reproductive rig...   \n",
       "     4        Crime, law enforcement, gun control, drugs, al...   \n",
       "     5        Budget, debt, deficit, government spending, so...   \n",
       "     6        Foreign policy, national security, nuclear wea...   \n",
       "     7        Globalization, trade, international cooperatio...   \n",
       "     8        Environment (general), climate change, polluti...   \n",
       "     9        Morals and values, family values, religion, cu...   \n",
       "     10       Politics, corruption, trustworthiness, leaders...   \n",
       "     11       Youths (general), crime, gangs, school violenc...   \n",
       "     12       Native Americans, hispanics, Asian Americans, ...   \n",
       "     13                                              Other, all   \n",
       "     14                                                     NaN   \n",
       "     15         Don't know, no opinion, not applicable, refusal   \n",
       "\n",
       "                                                problemexamples       perc  \n",
       "year problem                                                                \n",
       "2017 1        Jobs, cost of living, prices, interest rates, ...  19.669001  \n",
       "     2        Government programs, welfare, entitlements, ai...  16.531000  \n",
       "     3        Sexual harassment, gender equality, wage gap, ...   4.969000  \n",
       "     4        Crime, personal security, police, rule of law,...   1.275000  \n",
       "     5        Balance the budget, spending priorities, debt ...  15.047000  \n",
       "     6        Foreign affairs, international problems, forei...  11.080000  \n",
       "     7        Foreign ownership, foreign competition, trade ...   0.222000  \n",
       "     8        Environmental protection, conservation, climat...   2.157000  \n",
       "     9        Morals, values, ethics, honesty, family breakd...   5.814000  \n",
       "     10       Politics as usual, abuse of power, greedy poli...  14.678000  \n",
       "     11       Children, morals for youths, juvenile delinque...   0.426000  \n",
       "     12       Minorities, American Indians, anti-semitism, M...   0.000000  \n",
       "     13       Other, stress, stem cell research, computers a...   3.186000  \n",
       "     14       Nothing in particular, no other mention, no pr...   0.269000  \n",
       "     15       Not sure, can't say, not interested, unknown, ...   4.676000  \n",
       "2018 1        Jobs, cost of living, prices, interest rates, ...  14.319000  \n",
       "     2        Government programs, welfare, entitlements, ai...  11.224000  \n",
       "     3        Sexual harassment, gender equality, wage gap, ...   8.407000  \n",
       "     4        Crime, personal security, police, rule of law,...   3.457000  \n",
       "     5        Balance the budget, spending priorities, debt ...  15.009000  \n",
       "     6        Foreign affairs, international problems, forei...  12.685000  \n",
       "     7        Foreign ownership, foreign competition, trade ...   0.282000  \n",
       "     8        Environmental protection, conservation, climat...   1.959000  \n",
       "     9        Morals, values, ethics, honesty, family breakd...   5.000000  \n",
       "     10       Politics as usual, abuse of power, greedy poli...  17.754000  \n",
       "     11       Children, morals for youths, juvenile delinque...   0.707000  \n",
       "     12       Minorities, American Indians, anti-semitism, M...   0.000000  \n",
       "     13       Other, stress, stem cell research, computers a...   4.345000  \n",
       "     14       Nothing in particular, no other mention, no pr...   0.125000  \n",
       "     15       Not sure, can't say, not interested, unknown, ...   4.728000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_data[[\"problemname\",\"problemdesc\",\"problemissues\",\"problemexamples\",\"perc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79c87f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_code_dict = {'animal rights':14, \n",
    "                   'banking and finance':1, \n",
    "                   'civil rights':3,\n",
    "                   'corruption':10, \n",
    "                   'criminal justice':3, \n",
    "                   'democracy':10,\n",
    "                   'development':1, \n",
    "                   'disability rights':3, \n",
    "                   'drugs':4, \n",
    "                   'economy':1,\n",
    "                   'education':2, \n",
    "                   'energy':8, \n",
    "                   'environment':8, \n",
    "                   'foreign affairs':6,\n",
    "                   'free speech':3, \n",
    "                   'guns':4, \n",
    "                   'healthcare':2, \n",
    "                   'housing':2, \n",
    "                   'immigration':5,\n",
    "                   'indigenous peoples':12, \n",
    "                   'judiciary':10, \n",
    "                   'labor':1, \n",
    "                   'legislative':10,\n",
    "                   'lgbtqia':3, \n",
    "                   'military':6, \n",
    "                   'patriotism':9, \n",
    "                   'policing':3,\n",
    "                   'presidency':10,\n",
    "                   'racism':3, \n",
    "                   'religion':9, \n",
    "                   'reproductive rights':3, \n",
    "                   'science':10,\n",
    "                   'sexual violence':4, \n",
    "                   'sports':13, \n",
    "                   'taxes':1, \n",
    "                   'transportation':2,\n",
    "                   \"women's rights\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f5372ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DP02PR_0001PE</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0081PE</th>\n",
       "      <th>DP05_0082PE</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "      <th>problems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>[policing]</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2534993</td>\n",
       "      <td>3924471</td>\n",
       "      <td>47.4</td>\n",
       "      <td>52.6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[education, racism]</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>11460</td>\n",
       "      <td>Ann Arbor, MI Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150397</td>\n",
       "      <td>278978</td>\n",
       "      <td>49.2</td>\n",
       "      <td>50.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 679 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  fips_code                  type  \\\n",
       "0  2017    27053.0  protest; banner drop   \n",
       "0  2017    27053.0  protest; banner drop   \n",
       "0  2017    27053.0  protest; banner drop   \n",
       "1  2017    12086.0         demonstration   \n",
       "2  2017    26161.0               protest   \n",
       "\n",
       "                                              issues  size_mean  size_cat  \\\n",
       "0  [banking and finance, economy, energy, environ...        2.0         1   \n",
       "0  [banking and finance, economy, energy, environ...        2.0         1   \n",
       "0  [banking and finance, economy, energy, environ...        2.0         1   \n",
       "1                                         [policing]       18.0         1   \n",
       "2                                [education, racism]      200.0         2   \n",
       "\n",
       "   FIPS Code CBSA Code                                               NAME  \\\n",
       "0    27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "0    27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "0    27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "1    12086.0     33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "2    26161.0     11460                           Ann Arbor, MI Metro Area   \n",
       "\n",
       "   DP02PR_0001PE  ...  DP05_0081PE  DP05_0082PE  DP05_0083PE  DP05_0084PE  \\\n",
       "0            NaN  ...          0.0          0.1          2.7          0.1   \n",
       "0            NaN  ...          0.0          0.1          2.7          0.1   \n",
       "0            NaN  ...          0.0          0.1          2.7          0.1   \n",
       "1            NaN  ...          0.0          0.4          1.3          0.2   \n",
       "2            NaN  ...          0.0          0.3          4.1          0.1   \n",
       "\n",
       "   DP05_0085PE  DP05_0086E  DP05_0087PE  DP05_0088PE  DP05_0089PE  problems  \n",
       "0          2.6     1447758      2591001         49.0         51.0         1  \n",
       "0          2.6     1447758      2591001         49.0         51.0         8  \n",
       "0          2.6     1447758      2591001         49.0         51.0        12  \n",
       "1          1.1     2534993      3924471         47.4         52.6         3  \n",
       "2          4.0      150397       278978         49.2         50.8         2  \n",
       "\n",
       "[5 rows x 679 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data[\"problems\"] = crowd_census_data[\"issues\"].apply(np.vectorize(lambda x: issue_code_dict[x])).apply(np.unique)\n",
    "crowd_census_data_exploded = crowd_census_data.explode(column=\"problems\")\n",
    "crowd_census_data_exploded.head()\n",
    "\n",
    "# crowd_census_data_issues_exploded = crowd_census_data[\"issues\"].explode()\n",
    "# problems_onehot = pd.get_dummies(crowd_census_data_problems_exploded).groupby(crowd_census_data_problems_exploded.index).any()\n",
    "# problems_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96613620",
   "metadata": {},
   "source": [
    "Now, this may be merged with the issue data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dcb0f",
   "metadata": {},
   "source": [
    "We want to keep the index, however, so that we may group by it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5542f6",
   "metadata": {},
   "source": [
    "crowd_census_data_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df4e7fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>issues</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "      <th>problems</th>\n",
       "      <th>perc</th>\n",
       "      <th>problemname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.669001</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.157000</td>\n",
       "      <td>Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>[banking and finance, economy, energy, environ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>[policing]</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2534993</td>\n",
       "      <td>3924471</td>\n",
       "      <td>47.4</td>\n",
       "      <td>52.6</td>\n",
       "      <td>3</td>\n",
       "      <td>4.969000</td>\n",
       "      <td>Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[education, racism]</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>11460</td>\n",
       "      <td>Ann Arbor, MI Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150397</td>\n",
       "      <td>278978</td>\n",
       "      <td>49.2</td>\n",
       "      <td>50.8</td>\n",
       "      <td>2</td>\n",
       "      <td>16.531000</td>\n",
       "      <td>Social Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22236</th>\n",
       "      <td>14881</td>\n",
       "      <td>2018</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[education, racism]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>20500</td>\n",
       "      <td>Durham-Chapel Hill, NC Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>249694</td>\n",
       "      <td>416987</td>\n",
       "      <td>46.7</td>\n",
       "      <td>53.3</td>\n",
       "      <td>2</td>\n",
       "      <td>11.224000</td>\n",
       "      <td>Social Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22237</th>\n",
       "      <td>14881</td>\n",
       "      <td>2018</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[education, racism]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>20500</td>\n",
       "      <td>Durham-Chapel Hill, NC Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>249694</td>\n",
       "      <td>416987</td>\n",
       "      <td>46.7</td>\n",
       "      <td>53.3</td>\n",
       "      <td>3</td>\n",
       "      <td>8.407000</td>\n",
       "      <td>Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22238</th>\n",
       "      <td>14882</td>\n",
       "      <td>2018</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[foreign affairs]</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>24660</td>\n",
       "      <td>Greensboro-High Point, NC Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>337386</td>\n",
       "      <td>558753</td>\n",
       "      <td>46.4</td>\n",
       "      <td>53.6</td>\n",
       "      <td>6</td>\n",
       "      <td>12.685000</td>\n",
       "      <td>Foreign Policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22239</th>\n",
       "      <td>14883</td>\n",
       "      <td>2018</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[policing, racism]</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>33860</td>\n",
       "      <td>Montgomery, AL Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>168054</td>\n",
       "      <td>279570</td>\n",
       "      <td>46.6</td>\n",
       "      <td>53.4</td>\n",
       "      <td>3</td>\n",
       "      <td>8.407000</td>\n",
       "      <td>Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22240</th>\n",
       "      <td>14884</td>\n",
       "      <td>2018</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>[housing]</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>16980</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI Metro Area</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3858653</td>\n",
       "      <td>6589474</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.224000</td>\n",
       "      <td>Social Policy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22241 rows × 682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  year  fips_code                  type  \\\n",
       "0          0  2017    27053.0  protest; banner drop   \n",
       "1          0  2017    27053.0  protest; banner drop   \n",
       "2          0  2017    27053.0  protest; banner drop   \n",
       "3          1  2017    12086.0         demonstration   \n",
       "4          2  2017    26161.0               protest   \n",
       "...      ...   ...        ...                   ...   \n",
       "22236  14881  2018    37135.0               protest   \n",
       "22237  14881  2018    37135.0               protest   \n",
       "22238  14882  2018    37081.0               protest   \n",
       "22239  14883  2018     1101.0               protest   \n",
       "22240  14884  2018    17031.0               protest   \n",
       "\n",
       "                                                  issues  size_mean  size_cat  \\\n",
       "0      [banking and finance, economy, energy, environ...        2.0         1   \n",
       "1      [banking and finance, economy, energy, environ...        2.0         1   \n",
       "2      [banking and finance, economy, energy, environ...        2.0         1   \n",
       "3                                             [policing]       18.0         1   \n",
       "4                                    [education, racism]      200.0         2   \n",
       "...                                                  ...        ...       ...   \n",
       "22236                                [education, racism]        2.0         1   \n",
       "22237                                [education, racism]        2.0         1   \n",
       "22238                                  [foreign affairs]      150.0         2   \n",
       "22239                                 [policing, racism]       12.0         1   \n",
       "22240                                          [housing]       20.0         1   \n",
       "\n",
       "       FIPS Code CBSA Code                                               NAME  \\\n",
       "0        27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "1        27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "2        27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "3        12086.0     33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "4        26161.0     11460                           Ann Arbor, MI Metro Area   \n",
       "...          ...       ...                                                ...   \n",
       "22236    37135.0     20500                  Durham-Chapel Hill, NC Metro Area   \n",
       "22237    37135.0     20500                  Durham-Chapel Hill, NC Metro Area   \n",
       "22238    37081.0     24660               Greensboro-High Point, NC Metro Area   \n",
       "22239     1101.0     33860                          Montgomery, AL Metro Area   \n",
       "22240    17031.0     16980      Chicago-Naperville-Elgin, IL-IN-WI Metro Area   \n",
       "\n",
       "       ...  DP05_0083PE  DP05_0084PE  DP05_0085PE  DP05_0086E  DP05_0087PE  \\\n",
       "0      ...          2.7          0.1          2.6     1447758      2591001   \n",
       "1      ...          2.7          0.1          2.6     1447758      2591001   \n",
       "2      ...          2.7          0.1          2.6     1447758      2591001   \n",
       "3      ...          1.3          0.2          1.1     2534993      3924471   \n",
       "4      ...          4.1          0.1          4.0      150397       278978   \n",
       "...    ...          ...          ...          ...         ...          ...   \n",
       "22236  ...          2.5          0.1          2.4      249694       416987   \n",
       "22237  ...          2.5          0.1          2.4      249694       416987   \n",
       "22238  ...          2.5          0.1          2.4      337386       558753   \n",
       "22239  ...          1.6          0.1          1.5      168054       279570   \n",
       "22240  ...          2.0          0.1          1.9     3858653      6589474   \n",
       "\n",
       "       DP05_0088PE  DP05_0089PE  problems       perc     problemname  \n",
       "0             49.0         51.0         1  19.669001         Economy  \n",
       "1             49.0         51.0         8   2.157000     Environment  \n",
       "2             49.0         51.0        12   0.000000          Groups  \n",
       "3             47.4         52.6         3   4.969000          Rights  \n",
       "4             49.2         50.8         2  16.531000   Social Policy  \n",
       "...            ...          ...       ...        ...             ...  \n",
       "22236         46.7         53.3         2  11.224000   Social Policy  \n",
       "22237         46.7         53.3         3   8.407000          Rights  \n",
       "22238         46.4         53.6         6  12.685000  Foreign Policy  \n",
       "22239         46.6         53.4         3   8.407000          Rights  \n",
       "22240         48.0         52.0         2  11.224000   Social Policy  \n",
       "\n",
       "[22241 rows x 682 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowd_census_data_exploded_to_merge = crowd_census_data_exploded.reset_index()\n",
    "\n",
    "demo_data_exploded = crowd_census_data_exploded_to_merge.merge(issue_data[[\"perc\",\"problemname\"]], how=\"inner\", left_on=[\"year\",\"problems\"], right_on=[\"year\",\"problem\"])\n",
    "demo_data_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cc9e3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_45748\\3685731779.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>fips_code</th>\n",
       "      <th>type</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_cat</th>\n",
       "      <th>FIPS Code</th>\n",
       "      <th>CBSA Code</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DP02PR_0001PE</th>\n",
       "      <th>...</th>\n",
       "      <th>DP05_0081PE</th>\n",
       "      <th>DP05_0082PE</th>\n",
       "      <th>DP05_0083PE</th>\n",
       "      <th>DP05_0084PE</th>\n",
       "      <th>DP05_0085PE</th>\n",
       "      <th>DP05_0086E</th>\n",
       "      <th>DP05_0087PE</th>\n",
       "      <th>DP05_0088PE</th>\n",
       "      <th>DP05_0089PE</th>\n",
       "      <th>perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>protest; banner drop</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27053.0</td>\n",
       "      <td>33460</td>\n",
       "      <td>Minneapolis-St. Paul-Bloomington, MN-WI Metro ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1447758</td>\n",
       "      <td>2591001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>21.826001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12086.0</td>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2534993</td>\n",
       "      <td>3924471</td>\n",
       "      <td>47.4</td>\n",
       "      <td>52.6</td>\n",
       "      <td>4.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>26161.0</td>\n",
       "      <td>11460</td>\n",
       "      <td>Ann Arbor, MI Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150397</td>\n",
       "      <td>278978</td>\n",
       "      <td>49.2</td>\n",
       "      <td>50.8</td>\n",
       "      <td>21.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36061.0</td>\n",
       "      <td>35620</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7983770</td>\n",
       "      <td>13563604</td>\n",
       "      <td>47.2</td>\n",
       "      <td>52.8</td>\n",
       "      <td>14.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>47900</td>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2354827</td>\n",
       "      <td>4155063</td>\n",
       "      <td>48.2</td>\n",
       "      <td>51.8</td>\n",
       "      <td>14.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14880</th>\n",
       "      <td>14880</td>\n",
       "      <td>2018</td>\n",
       "      <td>12099.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12099.0</td>\n",
       "      <td>33100</td>\n",
       "      <td>Miami-Fort Lauderdale-West Palm Beach, FL Metr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2548910</td>\n",
       "      <td>3988782</td>\n",
       "      <td>47.4</td>\n",
       "      <td>52.6</td>\n",
       "      <td>17.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14881</th>\n",
       "      <td>14881</td>\n",
       "      <td>2018</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37135.0</td>\n",
       "      <td>20500</td>\n",
       "      <td>Durham-Chapel Hill, NC Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>249694</td>\n",
       "      <td>416987</td>\n",
       "      <td>46.7</td>\n",
       "      <td>53.3</td>\n",
       "      <td>19.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14882</th>\n",
       "      <td>14882</td>\n",
       "      <td>2018</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37081.0</td>\n",
       "      <td>24660</td>\n",
       "      <td>Greensboro-High Point, NC Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>337386</td>\n",
       "      <td>558753</td>\n",
       "      <td>46.4</td>\n",
       "      <td>53.6</td>\n",
       "      <td>12.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14883</th>\n",
       "      <td>14883</td>\n",
       "      <td>2018</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>33860</td>\n",
       "      <td>Montgomery, AL Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>168054</td>\n",
       "      <td>279570</td>\n",
       "      <td>46.6</td>\n",
       "      <td>53.4</td>\n",
       "      <td>8.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14884</th>\n",
       "      <td>14884</td>\n",
       "      <td>2018</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>protest</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>16980</td>\n",
       "      <td>Chicago-Naperville-Elgin, IL-IN-WI Metro Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3858653</td>\n",
       "      <td>6589474</td>\n",
       "      <td>48.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.224000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14885 rows × 679 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  year  fips_code                  type  size_mean  size_cat  \\\n",
       "0          0  2017    27053.0  protest; banner drop        2.0         1   \n",
       "1          1  2017    12086.0         demonstration       18.0         1   \n",
       "2          2  2017    26161.0               protest      200.0         2   \n",
       "3          3  2017    36061.0               protest        2.0         1   \n",
       "4          4  2017    11001.0               protest        3.0         1   \n",
       "...      ...   ...        ...                   ...        ...       ...   \n",
       "14880  14880  2018    12099.0               protest        1.0         1   \n",
       "14881  14881  2018    37135.0               protest        2.0         1   \n",
       "14882  14882  2018    37081.0               protest      150.0         2   \n",
       "14883  14883  2018     1101.0               protest       12.0         1   \n",
       "14884  14884  2018    17031.0               protest       20.0         1   \n",
       "\n",
       "       FIPS Code CBSA Code                                               NAME  \\\n",
       "0        27053.0     33460  Minneapolis-St. Paul-Bloomington, MN-WI Metro ...   \n",
       "1        12086.0     33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "2        26161.0     11460                           Ann Arbor, MI Metro Area   \n",
       "3        36061.0     35620   New York-Newark-Jersey City, NY-NJ-PA Metro Area   \n",
       "4        11001.0     47900  Washington-Arlington-Alexandria, DC-VA-MD-WV M...   \n",
       "...          ...       ...                                                ...   \n",
       "14880    12099.0     33100  Miami-Fort Lauderdale-West Palm Beach, FL Metr...   \n",
       "14881    37135.0     20500                  Durham-Chapel Hill, NC Metro Area   \n",
       "14882    37081.0     24660               Greensboro-High Point, NC Metro Area   \n",
       "14883     1101.0     33860                          Montgomery, AL Metro Area   \n",
       "14884    17031.0     16980      Chicago-Naperville-Elgin, IL-IN-WI Metro Area   \n",
       "\n",
       "       DP02PR_0001PE  ...  DP05_0081PE  DP05_0082PE  DP05_0083PE  DP05_0084PE  \\\n",
       "0                NaN  ...          0.0          0.1          2.7          0.1   \n",
       "1                NaN  ...          0.0          0.4          1.3          0.2   \n",
       "2                NaN  ...          0.0          0.3          4.1          0.1   \n",
       "3                NaN  ...          0.0          0.7          1.7          0.2   \n",
       "4                NaN  ...          0.0          0.4          3.3          0.2   \n",
       "...              ...  ...          ...          ...          ...          ...   \n",
       "14880            NaN  ...          0.0          0.4          1.4          0.1   \n",
       "14881            NaN  ...          0.0          0.4          2.5          0.1   \n",
       "14882            NaN  ...          0.0          0.5          2.5          0.1   \n",
       "14883            NaN  ...          0.0          0.1          1.6          0.1   \n",
       "14884            NaN  ...          0.0          0.2          2.0          0.1   \n",
       "\n",
       "       DP05_0085PE  DP05_0086E  DP05_0087PE  DP05_0088PE  DP05_0089PE  \\\n",
       "0              2.6     1447758      2591001         49.0         51.0   \n",
       "1              1.1     2534993      3924471         47.4         52.6   \n",
       "2              4.0      150397       278978         49.2         50.8   \n",
       "3              1.5     7983770     13563604         47.2         52.8   \n",
       "4              3.1     2354827      4155063         48.2         51.8   \n",
       "...            ...         ...          ...          ...          ...   \n",
       "14880          1.3     2548910      3988782         47.4         52.6   \n",
       "14881          2.4      249694       416987         46.7         53.3   \n",
       "14882          2.4      337386       558753         46.4         53.6   \n",
       "14883          1.5      168054       279570         46.6         53.4   \n",
       "14884          1.9     3858653      6589474         48.0         52.0   \n",
       "\n",
       "            perc  \n",
       "0      21.826001  \n",
       "1       4.969000  \n",
       "2      21.500000  \n",
       "3      14.678000  \n",
       "4      14.678000  \n",
       "...          ...  \n",
       "14880  17.754000  \n",
       "14881  19.631000  \n",
       "14882  12.685000  \n",
       "14883   8.407000  \n",
       "14884  11.224000  \n",
       "\n",
       "[14885 rows x 679 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_data = demo_data_exploded.drop(columns=[\"issues\",\"problems\",\"problemname\"])\n",
    "columns_not_perc = demo_data.columns\n",
    "columns_not_perc = list(columns_not_perc.drop(\"perc\"))\n",
    "columns_not_perc\n",
    "demo_data = demo_data.groupby(by=columns_not_perc, dropna=False).sum().reset_index()\n",
    "demo_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
